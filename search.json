[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "02360018_final_project",
    "section": "",
    "text": "If you are new to using nbdev here are some useful pointers to get you started.\n\n\n# make sure electricity_market package is installed in development mode\n$ pip install -e '.[dev]'\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to electricity_market\n$ nbdev_prepare",
    "crumbs": [
      "02360018_final_project"
    ]
  },
  {
    "objectID": "index.html#developer-guide",
    "href": "index.html#developer-guide",
    "title": "02360018_final_project",
    "section": "",
    "text": "If you are new to using nbdev here are some useful pointers to get you started.\n\n\n# make sure electricity_market package is installed in development mode\n$ pip install -e '.[dev]'\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to electricity_market\n$ nbdev_prepare",
    "crumbs": [
      "02360018_final_project"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "02360018_final_project",
    "section": "Usage",
    "text": "Usage\n\nInstallation\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/techofer/02360018_final_project.git\n\n\nDocumentation\nDocumentation can be found hosted on this GitHub repository’s pages.\n\n\nTrained Models\nPlaced in ./trained_models\n\n\nTensorBoard\ntensorboard --logdir=./tensorboard",
    "crumbs": [
      "02360018_final_project"
    ]
  },
  {
    "objectID": "env.html",
    "href": "env.html",
    "title": "env",
    "section": "",
    "text": "source\n\nWeather\n\n Weather (value, names=None, module=None, qualname=None, type=None,\n          start=1)\n\nAn enumeration.\n\nsource\n\n\nSeason\n\n Season (value, names=None, module=None, qualname=None, type=None,\n         start=1)\n\nAn enumeration.\n\nsource\n\n\nEnvConfig\n\n EnvConfig (max_timestep:int=17520, init_battery_capacity:int=250,\n            init_state_of_charge:int=200, production_capacity:float=120.0,\n            base_price:float=0.065, night_price_factor:float=1.2,\n            high_demand_seasons_price_factor:float=1.3,\n            night_demand_factor:float=1.1,\n            high_demand_seasons_demand_factor:float=1.2,\n            base_demand_of_electricity:int=75,\n            battery_safe_range_ratios:tuple[float,float]=(0.1, 0.9),\n            battery_degradation_factor:float=0.9999,\n            battery_unsafe_degradation_exponent:int=10,\n            battery_capacity_ratio_for_termination:float=0.2)\n\n\n\nExported source\n@dataclass\nclass EnvConfig:\n    max_timestep: int = 365 * 6 * 8  # default: 8 year in timesteps\n    init_battery_capacity: int = 250  # default: 25 kWh\n    init_state_of_charge: int = 200  # default: 20 kWh\n    production_capacity: float = 720 / 6  # default: 72 kWh/day\n    base_price: float = 0.065  # 0.65 NIS per kWh\n    night_price_factor: float = 1.2\n    high_demand_seasons_price_factor: float = 1.3\n    night_demand_factor: float = 1.1\n    high_demand_seasons_demand_factor: float = 1.2\n    base_demand_of_electricity: int = 75  # 75 * 100 Wh per timestep\n    # battery safe range\n    battery_safe_range_ratios: tuple[float, float] = (0.1, 0.9)\n    battery_degradation_factor: float = 0.9999\n    # degradation when unsafe is 10 time faster\n    battery_unsafe_degradation_exponent: int = 10\n    battery_capacity_ratio_for_termination: float = 0.2\n\n\n\nsource\n\n\nElectricityMarketEnv\n\n ElectricityMarketEnv (env_config:__main__.EnvConfig|None=None,\n                       render_mode:str|None=None)\n\n*The main Gymnasium class for implementing Reinforcement Learning Agents environments.\nThe class encapsulates an environment with arbitrary behind-the-scenes dynamics through the :meth:step and :meth:reset functions. An environment can be partially or fully observed by single agents. For multi-agent environments, see PettingZoo.\nThe main API methods that users of this class need to know are:\n\n:meth:step - Updates an environment with actions returning the next agent observation, the reward for taking that actions, if the environment has terminated or truncated due to the latest action and information from the environment about the step, i.e. metrics, debug info.\n:meth:reset - Resets the environment to an initial state, required before calling step. Returns the first agent observation for an episode and information, i.e. metrics, debug info.\n:meth:render - Renders the environments to help visualise what the agent see, examples modes are “human”, “rgb_array”, “ansi” for text.\n:meth:close - Closes the environment, important when external software is used, i.e. pygame for rendering, databases\n\nEnvironments have additional attributes for users to understand the implementation\n\n:attr:action_space - The Space object corresponding to valid actions, all valid actions should be contained within the space.\n:attr:observation_space - The Space object corresponding to valid observations, all valid observations should be contained within the space.\n:attr:spec - An environment spec that contains the information used to initialize the environment from :meth:gymnasium.make\n:attr:metadata - The metadata of the environment, e.g. {\"render_modes\": [\"rgb_array\", \"human\"], \"render_fps\": 30}. For Jax or Torch, this can be indicated to users with \"jax\"=True or \"torch\"=True.\n:attr:np_random - The random number generator for the environment. This is automatically assigned during super().reset(seed=seed) and when assessing :attr:np_random.\n\n.. seealso:: For modifying or extending environments use the :class:gymnasium.Wrapper class\nNote: To get reproducible sampling of actions, a seed can be set with env.action_space.seed(123).\nNote: For strict type checking (e.g. mypy or pyright), :class:Env is a generic class with two parameterized types: ObsType and ActType. The ObsType and ActType are the expected types of the observations and actions used in :meth:reset and :meth:step. The environment’s :attr:observation_space and :attr:action_space should have type Space[ObsType] and Space[ActType], see a space’s implementation to find its parameterized type.*\n\n\nExported source\nclass ElectricityMarketEnv(gym.Env):\n    def __init__(\n        self, env_config: EnvConfig | None = None, render_mode: str | None = None\n    ):\n        if env_config is None:\n            env_config = EnvConfig()\n        self._config = env_config\n        self.render_mode = render_mode\n        # each timestep is 4 hours\n        self._timestep_duration_in_hours = 4\n        self._episode_obs: list = []\n        self._timestep = 0\n        # Decided On granularity of 100 Wh\n        self._battery_capacity = self._config.init_battery_capacity\n        self._init_state_of_charge = self._config.init_state_of_charge\n        self._current_state_of_charge = self._init_state_of_charge\n        self._production_capacity = self._config.production_capacity\n        self._max_timestep = self._config.max_timestep\n\n        self._base_price = self._config.base_price\n        self._night_price_factor = self._config.night_price_factor\n        self._high_demand_seasons_price_factor = (\n            self._config.high_demand_seasons_price_factor\n        )\n        self._battery_degradation_factor = self._config.battery_degradation_factor\n        self._battery_unsafe_degradation_exponent = (\n            self._config.battery_unsafe_degradation_exponent\n        )\n\n        self._battery_capacity_ratio_for_termination = (\n            self._config.battery_capacity_ratio_for_termination\n        )\n        self._night_demand_factor = self._config.night_demand_factor\n        self._high_demand_seasons_demand_factor = (\n            self._config.high_demand_seasons_demand_factor\n        )\n        self._base_demand_of_electricity = self._config.base_demand_of_electricity\n\n        self._battery_safe_range_ratios = self._config.battery_safe_range_ratios\n\n        self.__weather = self._get_weather()\n        self.__sell_price = self._get_sell_price()\n        self.__demand_of_electricity = self._get_demand_of_electricity()\n        self.action_space = gym.spaces.Discrete(2 * self._battery_capacity + 1)\n        self.actions = list(range(-self._battery_capacity, self._battery_capacity + 1))\n        self.observation_space = gym.spaces.Box(\n            low=np.array([0, 0, 0, 0, 0, 0, 0, 0]),\n            high=np.array(\n                [\n                    # Battery SoC\n                    self._current_state_of_charge / self._battery_capacity,\n                    # current battery capacity\n                    self._battery_capacity / self._battery_capacity,\n                    # battery safe range low boundary\n                    self._battery_safe_range[0] / self._battery_capacity,\n                    # battery safe range high boundary\n                    self._battery_safe_range[1] / self._battery_capacity,\n                    # Current electricity demand,\n                    self._max_demand_of_electricity / self._max_demand_of_electricity,\n                    # Current market price\n                    self._max_price / self._max_price,\n                    # Safe range indicator: 1 if safe, 0 if violated\n                    1,\n                    # production\n                    self._production_capacity / self._production_capacity,\n                ]\n            ),\n            shape=(8,),\n            dtype=np.float64,\n        )\n\n    def _charge_amount(self, action: int) -&gt; int:\n        return math.ceil(self.actions[action] - self._demand_of_electricity)\n\n    def _electricity_leftover(self, action: int) -&gt; float:\n        return self._production - self._charge_amount(action)\n\n    def _is_action_valid(self, action: int) -&gt; bool:\n        charge_amount = self._charge_amount(action)\n        if charge_amount &gt; self._production:\n            return False\n        target_state_of_charge = self._current_state_of_charge + charge_amount\n        return 0 &lt;= target_state_of_charge &lt;= self._battery_capacity\n\n    @property\n    def _is_done(self):\n        return (\n            self._battery_capacity\n            &lt;= self._config.init_battery_capacity\n            * self._battery_capacity_ratio_for_termination\n        )\n\n    def step(self, action: int) -&gt; tuple:\n        charge_amount = self._charge_amount(action)\n        truncated = self._timestep &gt;= self._max_timestep\n        done = self._is_done or truncated\n\n        if not self._is_action_valid(action):\n            reward = -1\n            self._timestep += 1\n            self.__weather = self._get_weather()\n            self.__sell_price = self._get_sell_price()\n            self.__demand_of_electricity = self._get_demand_of_electricity()\n            observations = self._get_obs()\n            self._episode_obs.append(observations)\n            return observations, reward, done, truncated, {}\n\n        self._current_state_of_charge += charge_amount\n        self._battery_capacity *= self._battery_degradation_factor\n        # if violated the safe range, extra degradation\n        if self._is_safe_range_violation:\n            self._battery_capacity *= (\n                self._battery_degradation_factor\n                ** self._battery_unsafe_degradation_exponent\n            )\n        reward = self._reward(action)\n        self._timestep += 1\n        self.__weather = self._get_weather()\n        self.__sell_price = self._get_sell_price()\n        self.__demand_of_electricity = self._get_demand_of_electricity()\n        observations = self._get_obs()\n        self._episode_obs.append(observations)\n        return observations, reward, done, truncated, {}\n\n    def _get_weather(self) -&gt; Weather:\n        options, probs = zip(\n            *WEATHER_PROBABILITIES_MAP_PER_SEASON[self._season].items()\n        )\n        return np.random.choice(options, p=probs)\n\n    @property\n    def _weather(self) -&gt; Weather:\n        return self.__weather\n\n    @property\n    def _is_dark_hours(self) -&gt; bool:\n        # assuming night is 20:00-08:00 (dark hours)\n        return self._timestep % 6 &lt; 2 or self._timestep % 6 &gt; 5\n\n    @property\n    def _season(self) -&gt; Season:\n        days_in_year = 365\n        hours_in_day = 24\n        number_of_seasons = 4\n        timesteps_in_day = hours_in_day / self._timestep_duration_in_hours\n        day_of_year = (self._timestep // timesteps_in_day) % days_in_year + 1\n        days_in_season = math.ceil(days_in_year / number_of_seasons)\n        match day_of_year // days_in_season:\n            case 0:\n                return Season.WINTER\n            case 1:\n                return Season.SPRING\n            case 2:\n                return Season.SUMMER\n            case 3:\n                return Season.FALL\n            case _:\n                raise ValueError(f\"{days_in_season=} {day_of_year=}\")\n\n    @property\n    def _production(self) -&gt; float:\n        # Solar panels doesn't produce at night.\n        if self._is_dark_hours:\n            return 0\n        # Solar panels doesn't produce well on cloudy days\n        match self._weather:\n            case Weather.SUNNY:\n                return self._production_capacity\n            case Weather.CLOUDY:\n                return self._production_capacity * 0.4\n            case Weather.PARTIAL_CLOUDY:\n                return self._production_capacity * 0.7\n            case _:\n                raise ValueError(\"Weather not supported\")\n\n    def _reward(self, action) -&gt; float:\n        # penalty for violating the safe range\n        if self._is_safe_range_violation:\n            return -0.8\n        sell_amount = self._electricity_leftover(action)\n        if sell_amount &lt; 0:\n            return -0.5\n        max_reward = self._max_price * (\n            self._config.init_battery_capacity\n            + self._production_capacity\n            - self._min_electricity_demand\n        )\n        reward = sell_amount * self._sell_price\n        # normalize (max reward is not reachable)\n        normalized_reward = reward / max_reward\n        return normalized_reward\n\n    @property\n    def _demand_of_electricity(self) -&gt; float:\n        return self.__demand_of_electricity\n\n    @property\n    def _min_electricity_demand(self) -&gt; float:\n        return self._base_demand_of_electricity * 0.8\n\n    @property\n    def _max_demand_of_electricity(self) -&gt; float:\n        return (\n            self._base_demand_of_electricity\n            * self._high_demand_seasons_demand_factor\n            * self._night_demand_factor\n            * 1.2\n        )\n\n    def _get_demand_of_electricity(self) -&gt; float:\n        demand = self._base_demand_of_electricity\n        if self._season in (Season.SUMMER, Season.WINTER):\n            demand *= self._high_demand_seasons_demand_factor\n        if self._is_dark_hours:\n            demand *= self._night_demand_factor\n        noise = np.random.uniform(-0.2 * demand, 0.2 * demand)\n        return demand + noise\n\n    @property\n    def _is_safe_range_violation(self) -&gt; bool:\n        low, high = self._battery_safe_range\n        return (\n            self._current_state_of_charge &lt; low or self._current_state_of_charge &gt; high\n        )\n\n    @property\n    def _battery_safe_range(self) -&gt; tuple[float, float]:\n        low, high = self._battery_safe_range_ratios\n        return low * self._battery_capacity, high * self._battery_capacity\n\n    @property\n    def _max_price(self) -&gt; float:\n        # base price in dark hours in winter/summer with max noise\n        return (\n            self._base_price\n            * self._night_price_factor\n            * self._high_demand_seasons_price_factor\n            * 1.2\n        )\n\n    def _get_sell_price(self) -&gt; float:\n        price = self._base_price\n        if self._is_dark_hours:\n            price *= self._night_price_factor\n        if self._season in (Season.WINTER, Season.SUMMER):\n            price *= self._high_demand_seasons_price_factor\n        noise = np.random.uniform(-price * 0.2, price * 0.2)\n        return price + noise\n\n    @property\n    def _sell_price(self) -&gt; float:\n        return self.__sell_price\n\n    def reset(self, *, seed: int | None = None, options: dict | None = None):\n        \"\"\"Resets the environment to the initial state.\"\"\"\n        np.random.seed(seed)\n        if seed is not None:\n            torch.manual_seed(seed)\n        super().reset(seed=seed, options=options)\n        self._timestep = 0\n        self._current_state_of_charge = self._init_state_of_charge\n        self._battery_capacity = self._config.init_battery_capacity\n        np.random.seed(seed)\n        self.__weather = self._get_weather()\n        self.__sell_price = self._get_sell_price()\n        self.__demand_of_electricity = self._get_demand_of_electricity()\n        observations = self._get_obs()\n        self._episode_obs = [observations]\n        return observations, {}\n\n    def action_masks(self) -&gt; np.ndarray:\n        \"\"\"Generate a boolean mask of valid actions for `MaskablePPO`.\"\"\"\n        mask = np.array(\n            [self._is_action_valid(action) for action in range(self.action_space.n)],\n            dtype=bool,\n        )\n        if not np.any(mask):  # If all actions are invalid, force one to be valid\n            mask[len(mask) // 2] = True\n        return mask\n\n    def _get_obs(self) -&gt; np.ndarray:\n        \"\"\"Returns the current observation (state).\"\"\"\n        safe_range_indicator = 1 if not self._is_safe_range_violation else 0\n        return np.array(\n            [\n                # Battery SoC\n                self._current_state_of_charge / self._config.init_battery_capacity,\n                # current battery capacity\n                self._battery_capacity / self._config.init_battery_capacity,\n                # battery safe range low boundary\n                self._battery_safe_range[0] / self._config.init_battery_capacity,\n                # battery safe range high boundary\n                self._battery_safe_range[1] / self._config.init_battery_capacity,\n                # Current electricity demand\n                self._demand_of_electricity / self._max_demand_of_electricity,\n                # Current market price\n                self._sell_price / self._max_price,\n                # Safe range indicator: 1 if safe, 0 if violated\n                safe_range_indicator,\n                # production\n                self._production / self._production_capacity,\n            ],\n            dtype=np.float64,\n        )\n\n    def render(self) -&gt; RenderFrame | list[RenderFrame] | None:\n        if not self._is_done and self._timestep &lt; self._config.max_timestep:\n            return []\n\n        observations = np.array(self._episode_obs)\n        timesteps = np.arange(len(observations))\n        feature_labels = [\"Battery Level\", \"Battery Capacity\"]\n\n        # Create a figure with two subplots: one for Battery Level & Electricity Demand, and one for Price\n        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 6))\n\n        # Plot Battery Level, Battery Capacity, and Electricity Demand in the first subplot (ax1)\n        ax1.plot(\n            timesteps, observations[:, 0], label=feature_labels[0]\n        )  # Battery Level (SoC)\n        ax1.plot(\n            timesteps, observations[:, 1], label=feature_labels[1]\n        )  # Battery Capacity\n\n        # Use the safe range boundaries from observations (indices 2 and 3)\n        low = observations[:, 2]  # Safe range low (from observation)\n        high = observations[:, 3]  # Safe range high (from observation)\n\n        # Plot the safe range boundaries (red lines)\n        ax1.plot(\n            timesteps, low, color=\"red\", linestyle=\"--\", label=\"Battery Safe Range Low\"\n        )\n        ax1.plot(\n            timesteps,\n            high,\n            color=\"red\",\n            linestyle=\"--\",\n            label=\"Battery Safe Range High\",\n        )\n\n        # Add labels, title, and legend for the first plot\n        ax1.set_xlabel(\"Timestep\")\n        ax1.set_ylabel(\"Value\")\n        ax1.set_title(\"Battery Level, Battery Capacity\")\n        ax1.legend()\n\n        # Plot the Price in the second subplot (ax2)\n        ax2.plot(\n            timesteps, observations[:, 5], label=\"Price\", color=\"orange\"\n        )  # Price from observation\n\n        # Optionally, customize the second plot scale (e.g., different y-limits)\n        ax2.set_ylim(bottom=0)  # Adjust this as needed for your price range\n\n        # Add labels, title, and legend for the second plot\n        ax2.set_xlabel(\"Timestep\")\n        ax2.set_ylabel(\"Price\")\n        ax2.set_title(\"Electricity Price Over Time\")\n        ax2.legend()\n\n        ax3.plot(\n            timesteps, observations[:, 4], label=\"Electricity Demand\", color=\"orange\"\n        )  # Price\n        ax3.set_ylim(bottom=0)  # Adjust this as needed for your price range\n\n        # Add labels, title, and legend for the second plot\n        ax3.set_xlabel(\"Timestep\")\n        ax3.set_ylabel(\"Demand\")\n        ax3.set_title(\"Electricity Demand Over Time\")\n        ax3.legend()\n\n        plt.pause(0.001)\n\n        # Convert plot to an image\n        buf = BytesIO()\n        plt.savefig(buf, format=\"png\")\n        plt.close()\n        buf.seek(0)\n\n        img = Image.open(buf)\n        return [img]",
    "crumbs": [
      "env"
    ]
  },
  {
    "objectID": "player.html",
    "href": "player.html",
    "title": "player",
    "section": "",
    "text": "source\n\nMaskableModelAgent\n\n MaskableModelAgent (name, env, env_config, model, device)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\n\nExported source\nclass Agent(ABC):\n    def __init__(self, name, env, device):\n        self.name = name\n        self.device = device\n        self.env = env\n\n    def evaluate(self, render: bool = False) -&gt; EvaluationData:\n        \"\"\"\n        Evaluate the model, and return EvaluationData.\n        \"\"\"\n        all_rewards = []\n\n        for seed in tqdm(EVALUATE_SEEDS, desc=\"seeds\"):\n            obs, _ = self.env.reset(seed=seed)\n            episode_rewards = []\n            done = False\n\n            while not done:\n                obs_tensor = torch.tensor(obs, dtype=torch.float64).to(self.device)\n\n                action = self.choose_action(obs_tensor)\n                obs, reward, done, truncated, _ = self.env.step(action)\n                episode_rewards.append(reward)\n\n                if render:\n                    self.env.render()\n\n                if truncated:\n                    break\n\n            all_rewards.append(np.sum(episode_rewards))\n\n        return EvaluationData(\n            episodes=list(range(len(all_rewards))),\n            rewards=all_rewards,\n        )\n\n    def choose_action(self, obs_tensor):\n        raise NotImplementedError\n\n\nclass ModelAgent(Agent):\n    def __init__(self, name, env, env_config, model, device):\n        super().__init__(name, device=device, env=env)\n        self.model = model\n        self.env_config = env_config\n\n    def train(self) -&gt; None:\n        \"\"\"\n        Train the model\n        \"\"\"\n        checkpoint_callback = CheckpointCallback(save_freq=1000, save_path=\"../logs/\")\n\n        for seed in tqdm(TRAIN_SEEDS, desc=\"seeds\"):\n            for _ in tqdm(range(N_TRAIN_EPISODES), desc=\"Training episodes\"):\n                self.env.reset(seed=seed)\n                self.model.learn(\n                    total_timesteps=self.env_config.max_timestep,\n                    callback=checkpoint_callback,\n                    reset_num_timesteps=False,\n                    tb_log_name=self.name,\n                )\n\n    def choose_action(self, obs_tensor):\n        action, _ = self.model.predict(obs_tensor, deterministic=True)\n        return action\n\n    def save_model(self, model_path: Path) -&gt; None:\n        self.model.save(str(model_path))\n\n    def load_model(self, model_path: Path) -&gt; None:\n        self.model = self.model.load(str(model_path), env=self.env)\n\n\nclass MaskableAgent(Agent):\n    @staticmethod\n    def mask_fn(env):\n        \"\"\"\n        Placeholder mask function if needed.\n        \"\"\"\n        return env.unwrapped.action_masks()\n\n\nclass MaskableModelAgent(MaskableAgent, ModelAgent):\n    def choose_action(self, obs_tensor):\n        action, _ = self.model.predict(\n            obs_tensor,\n            action_masks=MaskableAgent.mask_fn(self.env),\n            deterministic=True,\n        )\n\n\n\nsource\n\n\nMaskableAgent\n\n MaskableAgent (name, env, device)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nModelAgent\n\n ModelAgent (name, env, env_config, model, device)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nAgent\n\n Agent (name, env, device)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nMaskableRandomAgent\n\n MaskableRandomAgent\n                      (env_config:electricity_market.env.EnvConfig|None=No\n                      ne, render_mode:str|None=None,\n                      name:str='MaskableRandomAgent')\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\n\nExported source\nclass MaskableRandomAgent(MaskableAgent):\n    def __init__(\n        self,\n        env_config: EnvConfig | None = None,\n        render_mode: str | None = None,\n        name: str = \"MaskableRandomAgent\",\n    ):\n        \"\"\"\n        Initialize the agent and create the environment.\n        \"\"\"\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        env = ActionMasker(\n            ElectricityMarketEnv(env_config, render_mode=render_mode), self.mask_fn\n        )\n        super().__init__(name, device=device, env=env)\n\n    def choose_action(self, obs_tensor):\n        action_mask = self.env.action_masks()\n        valid_actions = np.where(action_mask)[0]\n        action = np.random.choice(valid_actions)\n\n        return action\n\n\n\nsource\n\n\nA2CAgent\n\n A2CAgent (env_config:electricity_market.env.EnvConfig|None=None,\n           render_mode:str|None=None, name:str='A2CAgent')\n\nA2C Agent for the Electricity Market Environment.\n\n\nExported source\nclass A2CAgent(ModelAgent):\n    \"\"\"A2C Agent for the Electricity Market Environment.\"\"\"\n\n    def __init__(\n        self,\n        env_config: EnvConfig | None = None,\n        render_mode: str | None = None,\n        name: str = \"A2CAgent\",\n    ):\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        env = Monitor(\n            ElectricityMarketEnv(env_config, render_mode=render_mode),\n        )\n        model = A2C(\n            \"MlpPolicy\",\n            env,\n            verbose=0,\n            tensorboard_log=f\"./tensorboard/\",\n            device=device,\n        )\n        super().__init__(\n            name=name, env=env, model=model, device=device, env_config=env_config\n        )\n\n\n\nsource\n\n\nMaskablePPOAgent\n\n MaskablePPOAgent (env_config:electricity_market.env.EnvConfig|None=None,\n                   render_mode:str|None=None, name:str='MaskablePPOAgent')\n\nMaskable PPO Agent for the Electricity Market Environment.\n\n\nExported source\nclass MaskablePPOAgent(ModelAgent, MaskableAgent):\n    \"\"\"Maskable PPO Agent for the Electricity Market Environment.\"\"\"\n\n    def __init__(\n        self,\n        env_config: EnvConfig | None = None,\n        render_mode: str | None = None,\n        name: str = \"MaskablePPOAgent\",\n    ):\n        env = Monitor(\n            ActionMasker(\n                ElectricityMarketEnv(env_config, render_mode=render_mode),\n                self.mask_fn,\n            )\n        )\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        model = MaskablePPO(\n            MaskableActorCriticPolicy,\n            env,\n            verbose=0,\n            tensorboard_log=f\"./tensorboard/\",\n            device=device,\n        )\n        super().__init__(\n            name=name, env=env, model=model, device=device, env_config=env_config\n        )\n        self.optimized_hyperparameters = {}\n        self.env_config = env_config or EnvConfig()\n\n    def choose_action(self, obs_tensor):\n        action, _ = self.model.predict(\n            obs_tensor, deterministic=True, action_masks=MaskableAgent.mask_fn(self.env)\n        )\n        return action\n\n    def optimize(self) -&gt; None:\n        \"\"\"\n        Optimize the agent with hyperparameters.\n        \"\"\"\n\n        def objective(trial):\n            learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n            n_steps = trial.suggest_int(\"n_steps\", 32, 1024, log=True)\n            batch_size = trial.suggest_int(\"batch_size\", 16, 256, log=True)\n            gae_lambda = trial.suggest_float(\"gae_lambda\", 0.8, 1.0)\n            ent_coef = trial.suggest_float(\"ent_coef\", 0.0, 0.02)\n            vf_coef = trial.suggest_float(\"vf_coef\", 0.1, 1.0)\n            clip_range = trial.suggest_float(\"clip_range\", 0.1, 0.3)\n            max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.1, 1.0)\n\n            agent = MaskablePPOAgent(\n                self.env_config,\n            )\n\n            model = MaskablePPO(\n                MaskableActorCriticPolicy,\n                agent.env,\n                learning_rate=learning_rate,\n                n_steps=n_steps,\n                batch_size=batch_size,\n                gae_lambda=gae_lambda,\n                ent_coef=ent_coef,\n                vf_coef=vf_coef,\n                clip_range=clip_range,\n                max_grad_norm=max_grad_norm,\n                verbose=0,\n                device=self.device,\n            )\n\n            agent.model = model\n            agent.train()\n\n            return np.mean(agent.evaluate().rewards)\n\n        study = optuna.create_study(\n            study_name=self.name,\n            storage=\"sqlite:///optuna_study.db\",\n            load_if_exists=True,\n            direction=\"maximize\",\n            pruner=optuna.pruners.HyperbandPruner(),\n            sampler=optuna.samplers.TPESampler(),\n        )\n\n        study.optimize(\n            objective,\n            n_trials=N_TRAILS,\n            n_jobs=-1,\n            show_progress_bar=True,\n            catch=(ValueError,),\n        )\n\n        self.optimized_hyperparameters = study.best_params\n\n        self.model = MaskablePPO(\n            MaskableActorCriticPolicy,\n            self.env,\n            **self.optimized_hyperparameters,\n            verbose=0,\n            tensorboard_log=f\"./tensorboard/\",\n            device=self.device,\n        )\n\n    def export_hyperparameters(self, filename: str):\n        \"\"\"\n        Export optimized learned_hyperparameters to a YAML file.\n        \"\"\"\n        with open(filename, \"w\") as file:\n            yaml.dump(self.optimized_hyperparameters, file)\n\n\n\n\nEvaluation MaskableRandom on ElectricityMarketEnv\n\n\nEvaluation A2C on ElectricityMarketEnv\n\n\nEvaluation MaskablePPO with default hyperparameters on ElectricityMarketEnv\n\n\nEvaluation MaskablePPO with optimized hyperparameters on ElectricityMarketEnv\n\n\nAdding expert knowledge to the masking function making learning more efficient\n\nsource\n\n\nexpert_knowledge_action_masks\n\n expert_knowledge_action_masks ()\n\n\n\nExported source\ndef is_action_safe(self, action: int) -&gt; bool:\n    charge_amount = self._charge_amount(action)\n    target_state_of_charge = self._current_state_of_charge + charge_amount\n    low, high = self._battery_safe_range\n    return high &gt; target_state_of_charge &gt; low\n\n\ndef expert_knowledge_action_masks(self) -&gt; np.ndarray:\n    mask = np.array(\n        [\n            self._is_action_valid(action) and self.is_action_safe(action)\n            for action in range(self.action_space.n)\n        ],\n        dtype=bool,\n    )\n    if not np.any(mask):\n        mask[len(mask) // 2] = True\n    return mask\n\n\n\nsource\n\n\nis_action_safe\n\n is_action_safe (action:int)\n\n\n\nEvaluation MaskableRandomAgent with Expert Knowledge on ElectricityMarketEnv\n\n\nEvaluation MaskablePPO with default hyperparameters and Expert Knowledge on ElectricityMarketEnv\n\n\nEvaluation MaskablePPO with optimized hyperparameters and Expert Knowledge on ElectricityMarketEnv",
    "crumbs": [
      "player"
    ]
  },
  {
    "objectID": "evaluation.html",
    "href": "evaluation.html",
    "title": "evaluation",
    "section": "",
    "text": "source\n\nplot_all_metrics\n\n plot_all_metrics\n                   (agent_eval_data:dict[str,electricity_market.utils.Eval\n                   uationData])",
    "crumbs": [
      "evaluation"
    ]
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "utils",
    "section": "",
    "text": "source\n\nEvaluationData\n\n EvaluationData (episodes:list[int], rewards:list[float])\n\n\n\nExported source\n@dataclass\nclass EvaluationData:\n    episodes: list[int]\n    rewards: list[float]",
    "crumbs": [
      "utils"
    ]
  }
]