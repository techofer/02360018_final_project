{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# player\n",
    "> description: This module training and optimizing RL agents on the electricity market\n",
    "  environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import pickle\n",
    "import shutil\n",
    "from abc import ABC\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "import yaml\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from electricity_market.env import ElectricityMarketEnv, EnvConfig\n",
    "from electricity_market.utils import EvaluationData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "N_TRAIN_EPISODES = 3\n",
    "N_TRAILS = 10\n",
    "TRAIN_SEEDS = [\n",
    "    111111,\n",
    "    121212,\n",
    "    123456,\n",
    "    200000,\n",
    "    217890,\n",
    "    222222,\n",
    "    224775,\n",
    "    234567,\n",
    "    253084,\n",
    "    285234,\n",
    "    312135,\n",
    "    314831,\n",
    "    333333,\n",
    "    345678,\n",
    "    406339,\n",
    "    444444,\n",
    "    471678,\n",
    "    555555,\n",
    "    562845,\n",
    "    666666,\n",
    "    701753,\n",
    "    755460,\n",
    "    761386,\n",
    "    777777,\n",
    "    789391,\n",
    "    888888,\n",
    "    993068,\n",
    "    979797,\n",
    "    987654,\n",
    "    999999,\n",
    "]\n",
    "EVALUATE_SEEDS = [\n",
    "    117127,\n",
    "    136901,\n",
    "    223246,\n",
    "    243382,\n",
    "    245720,\n",
    "    248832,\n",
    "    288598,\n",
    "    374487,\n",
    "    447331,\n",
    "    447851,\n",
    "    490428,\n",
    "    553737,\n",
    "    557309,\n",
    "    571504,\n",
    "    601426,\n",
    "    632202,\n",
    "    653634,\n",
    "    844596,\n",
    "    848937,\n",
    "    849735,\n",
    "    865470,\n",
    "    866822,\n",
    "    876563,\n",
    "    880689,\n",
    "    887591,\n",
    "    911016,\n",
    "    920528,\n",
    "    963993,\n",
    "    967995,\n",
    "    992634,\n",
    "]\n",
    "ENV_CONFIG = EnvConfig()\n",
    "\n",
    "TENSORBOARD_PATH = Path(\"../tensorboard\")\n",
    "LOGS_PATH = Path(\"../logs\")\n",
    "\n",
    "# Set QUICK_MODE = True for CI\n",
    "QUICK_MODE = True\n",
    "\n",
    "if QUICK_MODE:\n",
    "    ENV_CONFIG = EnvConfig(max_timestep=10)\n",
    "    TRAIN_SEEDS = [10000]\n",
    "    EVALUATE_SEEDS = [90000]\n",
    "    TENSORBOARD_PATH = None\n",
    "    LOGS_PATH = None\n",
    "\n",
    "evaluation_data_per_agent = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "# cleanup\n",
    "if not QUICK_MODE:\n",
    "    shutil.rmtree(TENSORBOARD_PATH, ignore_errors=True)\n",
    "    shutil.rmtree(LOGS_PATH, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "\n",
    "\n",
    "class Agent(ABC):\n",
    "    def __init__(self, name, env, device):\n",
    "        self.name = name\n",
    "        self.device = device\n",
    "        self.env = env\n",
    "\n",
    "    def evaluate(self, render: bool = False) -> EvaluationData:\n",
    "        \"\"\"\n",
    "        Evaluate the model, and return EvaluationData.\n",
    "        \"\"\"\n",
    "        all_rewards = []\n",
    "\n",
    "        for seed in tqdm(EVALUATE_SEEDS, desc=\"seeds\"):\n",
    "            obs, _ = self.env.reset(seed=seed)\n",
    "            episode_rewards = []\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                obs_tensor = torch.tensor(obs, dtype=torch.float64).to(self.device)\n",
    "\n",
    "                action = self.choose_action(obs_tensor)\n",
    "                obs, reward, done, truncated, _ = self.env.step(action)\n",
    "                episode_rewards.append(reward)\n",
    "\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "\n",
    "                if truncated:\n",
    "                    break\n",
    "\n",
    "            all_rewards.append(np.sum(episode_rewards))\n",
    "\n",
    "        return EvaluationData(\n",
    "            episodes=list(range(len(all_rewards))),\n",
    "            rewards=all_rewards,\n",
    "        )\n",
    "\n",
    "    def choose_action(self, obs_tensor):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class ModelAgent(Agent):\n",
    "    def __init__(self, name, env, env_config, model, device):\n",
    "        super().__init__(name, device=device, env=env)\n",
    "        self.model = model\n",
    "        self.env_config = env_config\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "        \"\"\"\n",
    "        checkpoint_callback = CheckpointCallback(save_freq=1000, save_path=\"../logs/\")\n",
    "\n",
    "        for seed in tqdm(TRAIN_SEEDS, desc=\"seeds\"):\n",
    "            for _ in tqdm(range(N_TRAIN_EPISODES), desc=\"Training episodes\"):\n",
    "                self.env.reset(seed=seed)\n",
    "                self.model.learn(\n",
    "                    total_timesteps=self.env_config.max_timestep,\n",
    "                    callback=checkpoint_callback,\n",
    "                    reset_num_timesteps=False,\n",
    "                    tb_log_name=self.name,\n",
    "                )\n",
    "\n",
    "    def choose_action(self, obs_tensor):\n",
    "        action, _ = self.model.predict(obs_tensor, deterministic=True)\n",
    "        return action\n",
    "\n",
    "    def save_model(self, model_path: Path) -> None:\n",
    "        self.model.save(str(model_path))\n",
    "\n",
    "    def load_model(self, model_path: Path) -> None:\n",
    "        self.model = self.model.load(str(model_path), env=self.env)\n",
    "\n",
    "\n",
    "class MaskableAgent(Agent):\n",
    "    @staticmethod\n",
    "    def mask_fn(env):\n",
    "        \"\"\"\n",
    "        Placeholder mask function if needed.\n",
    "        \"\"\"\n",
    "        return env.unwrapped.action_masks()\n",
    "\n",
    "\n",
    "class MaskableModelAgent(MaskableAgent, ModelAgent):\n",
    "    def choose_action(self, obs_tensor):\n",
    "        action, _ = self.model.predict(\n",
    "            obs_tensor,\n",
    "            action_masks=MaskableAgent.mask_fn(self.env),\n",
    "            deterministic=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "\n",
    "\n",
    "class MaskableRandomAgent(MaskableAgent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env_config: EnvConfig | None = None,\n",
    "        render_mode: str | None = None,\n",
    "        name: str = \"MaskableRandomAgent\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the agent and create the environment.\n",
    "        \"\"\"\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        env = ActionMasker(\n",
    "            ElectricityMarketEnv(env_config, render_mode=render_mode), self.mask_fn\n",
    "        )\n",
    "        super().__init__(name, device=device, env=env)\n",
    "\n",
    "    def choose_action(self, obs_tensor):\n",
    "        action_mask = self.env.action_masks()\n",
    "        valid_actions = np.where(action_mask)[0]\n",
    "        action = np.random.choice(valid_actions)\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "\n",
    "\n",
    "class A2CAgent(ModelAgent):\n",
    "    \"\"\"A2C Agent for the Electricity Market Environment.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env_config: EnvConfig | None = None,\n",
    "        render_mode: str | None = None,\n",
    "        name: str = \"A2CAgent\",\n",
    "    ):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        env = Monitor(\n",
    "            ElectricityMarketEnv(env_config, render_mode=render_mode),\n",
    "        )\n",
    "        model = A2C(\n",
    "            \"MlpPolicy\",\n",
    "            env,\n",
    "            verbose=0,\n",
    "            tensorboard_log=f\"./tensorboard/\",\n",
    "            device=device,\n",
    "        )\n",
    "        super().__init__(\n",
    "            name=name, env=env, model=model, device=device, env_config=env_config\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "\n",
    "\n",
    "class MaskablePPOAgent(ModelAgent, MaskableAgent):\n",
    "    \"\"\"Maskable PPO Agent for the Electricity Market Environment.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env_config: EnvConfig | None = None,\n",
    "        render_mode: str | None = None,\n",
    "        name: str = \"MaskablePPOAgent\",\n",
    "    ):\n",
    "        env = Monitor(\n",
    "            ActionMasker(\n",
    "                ElectricityMarketEnv(env_config, render_mode=render_mode),\n",
    "                self.mask_fn,\n",
    "            )\n",
    "        )\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model = MaskablePPO(\n",
    "            MaskableActorCriticPolicy,\n",
    "            env,\n",
    "            verbose=0,\n",
    "            tensorboard_log=f\"./tensorboard/\",\n",
    "            device=device,\n",
    "        )\n",
    "        super().__init__(\n",
    "            name=name, env=env, model=model, device=device, env_config=env_config\n",
    "        )\n",
    "        self.optimized_hyperparameters = {}\n",
    "        self.env_config = env_config or EnvConfig()\n",
    "\n",
    "    def choose_action(self, obs_tensor):\n",
    "        action, _ = self.model.predict(\n",
    "            obs_tensor, deterministic=True, action_masks=MaskableAgent.mask_fn(self.env)\n",
    "        )\n",
    "        return action\n",
    "\n",
    "    def optimize(self) -> None:\n",
    "        \"\"\"\n",
    "        Optimize the agent with hyperparameters.\n",
    "        \"\"\"\n",
    "\n",
    "        def objective(trial):\n",
    "            learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "            n_steps = trial.suggest_int(\"n_steps\", 32, 1024, log=True)\n",
    "            batch_size = trial.suggest_int(\"batch_size\", 16, 256, log=True)\n",
    "            gae_lambda = trial.suggest_float(\"gae_lambda\", 0.8, 1.0)\n",
    "            ent_coef = trial.suggest_float(\"ent_coef\", 0.0, 0.02)\n",
    "            vf_coef = trial.suggest_float(\"vf_coef\", 0.1, 1.0)\n",
    "            clip_range = trial.suggest_float(\"clip_range\", 0.1, 0.3)\n",
    "            max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.1, 1.0)\n",
    "\n",
    "            agent = MaskablePPOAgent(\n",
    "                self.env_config,\n",
    "            )\n",
    "\n",
    "            model = MaskablePPO(\n",
    "                MaskableActorCriticPolicy,\n",
    "                agent.env,\n",
    "                learning_rate=learning_rate,\n",
    "                n_steps=n_steps,\n",
    "                batch_size=batch_size,\n",
    "                gae_lambda=gae_lambda,\n",
    "                ent_coef=ent_coef,\n",
    "                vf_coef=vf_coef,\n",
    "                clip_range=clip_range,\n",
    "                max_grad_norm=max_grad_norm,\n",
    "                verbose=0,\n",
    "                device=self.device,\n",
    "            )\n",
    "\n",
    "            agent.model = model\n",
    "            agent.train()\n",
    "\n",
    "            return np.mean(agent.evaluate().rewards)\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            study_name=self.name,\n",
    "            storage=\"sqlite:///optuna_study.db\",\n",
    "            load_if_exists=True,\n",
    "            direction=\"maximize\",\n",
    "            pruner=optuna.pruners.HyperbandPruner(),\n",
    "            sampler=optuna.samplers.TPESampler(),\n",
    "        )\n",
    "\n",
    "        study.optimize(\n",
    "            objective,\n",
    "            n_trials=N_TRAILS,\n",
    "            n_jobs=-1,\n",
    "            show_progress_bar=True,\n",
    "            catch=(ValueError,),\n",
    "        )\n",
    "\n",
    "        self.optimized_hyperparameters = study.best_params\n",
    "\n",
    "        self.model = MaskablePPO(\n",
    "            MaskableActorCriticPolicy,\n",
    "            self.env,\n",
    "            **self.optimized_hyperparameters,\n",
    "            verbose=0,\n",
    "            tensorboard_log=f\"./tensorboard/\",\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "    def export_hyperparameters(self, filename: str):\n",
    "        \"\"\"\n",
    "        Export optimized learned_hyperparameters to a YAML file.\n",
    "        \"\"\"\n",
    "        with open(filename, \"w\") as file:\n",
    "            yaml.dump(self.optimized_hyperparameters, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation MaskableRandom on ElectricityMarketEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "maskable_random_agent = MaskableRandomAgent(render_mode=\"human\", env_config=ENV_CONFIG)\n",
    "\n",
    "evaluation_data_per_agent[maskable_random_agent.name] = maskable_random_agent.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation A2C on ElectricityMarketEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "a2c_agent = A2CAgent(render_mode=\"human\", env_config=ENV_CONFIG)\n",
    "\n",
    "a2c_agent.train()\n",
    "\n",
    "if not QUICK_MODE:\n",
    "    a2c_agent.save_model(f\"{a2c_agent.name}.model\")\n",
    "\n",
    "evaluation_data_per_agent[a2c_agent.name] = a2c_agent.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation MaskablePPO with default hyperparameters on ElectricityMarketEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "maskable_ppo_agent = MaskablePPOAgent(render_mode=\"human\", env_config=ENV_CONFIG)\n",
    "\n",
    "maskable_ppo_agent.train()\n",
    "\n",
    "if not QUICK_MODE:\n",
    "    maskable_ppo_agent.save_model(f\"{maskable_ppo_agent.name}.model\")\n",
    "\n",
    "evaluation_data_per_agent[maskable_ppo_agent.name] = maskable_ppo_agent.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation MaskablePPO with optimized hyperparameters on ElectricityMarketEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "optimized_maskable_ppo_agent = MaskablePPOAgent(\n",
    "    render_mode=\"human\", env_config=ENV_CONFIG, name=\"OptimizedMaskablePPOAgent\"\n",
    ")\n",
    "optimized_maskable_ppo_agent.optimize()\n",
    "\n",
    "if not QUICK_MODE:\n",
    "    optimized_maskable_ppo_agent.export_hyperparameters(\n",
    "        f\"{optimized_maskable_ppo_agent.name}.yaml\"\n",
    "    )\n",
    "\n",
    "optimized_maskable_ppo_agent.train()\n",
    "\n",
    "if not QUICK_MODE:\n",
    "    optimized_maskable_ppo_agent.save_model(\n",
    "        f\"{optimized_maskable_ppo_agent.name}.model\"\n",
    "    )\n",
    "\n",
    "evaluation_data_per_agent[optimized_maskable_ppo_agent.name] = (\n",
    "    optimized_maskable_ppo_agent.evaluate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding expert knowledge to the masking function making learning more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "\n",
    "\n",
    "def is_action_safe(self, action: int) -> bool:\n",
    "    charge_amount = self._charge_amount(action)\n",
    "    target_state_of_charge = self._current_state_of_charge + charge_amount\n",
    "    low, high = self._battery_safe_range\n",
    "    return high > target_state_of_charge > low\n",
    "\n",
    "\n",
    "def expert_knowledge_action_masks(self) -> np.ndarray:\n",
    "    mask = np.array(\n",
    "        [\n",
    "            self._is_action_valid(action) and self.is_action_safe(action)\n",
    "            for action in range(self.action_space.n)\n",
    "        ],\n",
    "        dtype=bool,\n",
    "    )\n",
    "    if not np.any(mask):\n",
    "        mask[len(mask) // 2] = True\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# Dynamically overriding action_masks to ElectricityMarketEnv\n",
    "setattr(ElectricityMarketEnv, \"action_masks\", expert_knowledge_action_masks)\n",
    "# Dynamically overriding injection is_action_safe to ElectricityMarketEnv\n",
    "setattr(ElectricityMarketEnv, \"is_action_safe\", is_action_safe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Evaluation MaskableRandomAgent with Expert Knowledge on ElectricityMarketEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "expert_maskable_random_agent = MaskableRandomAgent(\n",
    "    render_mode=\"human\", env_config=ENV_CONFIG, name=\"ExpertMaskableRandomAgent\"\n",
    ")\n",
    "\n",
    "evaluation_data_per_agent[expert_maskable_random_agent.name] = (\n",
    "    expert_maskable_random_agent.evaluate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation MaskablePPO with default hyperparameters and Expert Knowledge on ElectricityMarketEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "expert_maskable_ppo_agent = MaskablePPOAgent(\n",
    "    render_mode=\"human\", env_config=ENV_CONFIG, name=\"ExpertMaskablePPOAgent\"\n",
    ")\n",
    "\n",
    "expert_maskable_ppo_agent.train()\n",
    "\n",
    "\n",
    "if not QUICK_MODE:\n",
    "    expert_maskable_ppo_agent.save_model(f\"{expert_maskable_ppo_agent.name}.model\")\n",
    "\n",
    "evaluation_data_per_agent[expert_maskable_ppo_agent.name] = (\n",
    "    expert_maskable_ppo_agent.evaluate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation MaskablePPO with optimized hyperparameters and Expert Knowledge on ElectricityMarketEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "optimized_expert_maskable_ppo_agent = MaskablePPOAgent(\n",
    "    render_mode=\"human\", env_config=ENV_CONFIG, name=\"OptimizedExpertMaskablePPOAgent\"\n",
    ")\n",
    "optimized_expert_maskable_ppo_agent.optimize()\n",
    "\n",
    "if not QUICK_MODE:\n",
    "    optimized_expert_maskable_ppo_agent.export_hyperparameters(\n",
    "        f\"{optimized_expert_maskable_ppo_agent.name}.yaml\"\n",
    "    )\n",
    "\n",
    "optimized_expert_maskable_ppo_agent.train()\n",
    "\n",
    "if not QUICK_MODE:\n",
    "    optimized_expert_maskable_ppo_agent.save_model(\n",
    "        f\"{optimized_expert_maskable_ppo_agent.name}.model\"\n",
    "    )\n",
    "\n",
    "evaluation_data_per_agent[optimized_expert_maskable_ppo_agent.name] = (\n",
    "    optimized_expert_maskable_ppo_agent.evaluate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "if not QUICK_MODE:\n",
    "    with open(\"evaluation_data_per_agent.pkl\", \"wb\") as f:\n",
    "        pickle.dump(evaluation_data_per_agent, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
