{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation\n",
    "\n",
    "> This module evaluating RL agents on the electricity market environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import itertools\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import rliable.metrics\n",
    "import rliable.plot_utils\n",
    "import seaborn as sns\n",
    "\n",
    "from electricity_market.player import N_EPISODES, SEEDS\n",
    "from electricity_market.utils import EvaluationData, TrainingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def plot_all_metrics(\n",
    "    agent_train_data: dict[str, TrainingData],\n",
    "    agent_eval_data: dict[str, EvaluationData],\n",
    "):\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    def plot_learning_curve():\n",
    "        for agent, data in agent_train_data.items():\n",
    "            plt.plot(data.episodes, data.rewards, label=f\"{agent} Learning Curve\")\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.title(\"Learning Curves\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_training_stability():\n",
    "        for agent, data in agent_train_data.items():\n",
    "            plt.plot(\n",
    "                data.steps,\n",
    "                np.cumsum(data.rewards) / (np.arange(len(data.steps)) + 1),\n",
    "                label=f\"{agent} Stability\",\n",
    "            )\n",
    "        plt.xlabel(\"Steps\")\n",
    "        plt.ylabel(\"Cumulative Average Reward\")\n",
    "        plt.title(\"Training Stability\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_sample_efficiency():\n",
    "        for agent, data in agent_train_data.items():\n",
    "            plt.plot(\n",
    "                data.steps,\n",
    "                np.cumsum(data.rewards) / (np.arange(len(data.steps)) + 1),\n",
    "                label=f\"{agent} Sample Efficiency\",\n",
    "            )\n",
    "        plt.xlabel(\"Steps\")\n",
    "        plt.ylabel(\"Cumulative Average Reward\")\n",
    "        plt.title(\"Sample Efficiency Curve\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_time_to_convergence():\n",
    "        colors = sns.color_palette(\"tab10\", n_colors=len(agent_train_data))\n",
    "\n",
    "        for i, (agent, data) in enumerate(agent_train_data.items()):\n",
    "            if len(np.shape(data.rewards)) == 1:\n",
    "                converged_step = np.argmax(\n",
    "                    np.diff(data.rewards) < 0.01\n",
    "                )  # Example threshold for convergence\n",
    "                plt.axvline(\n",
    "                    x=data.steps[converged_step],\n",
    "                    color=colors[i],\n",
    "                    label=f\"{agent} Time to Convergence\",\n",
    "                )\n",
    "            else:\n",
    "                print(f\"Skipping {agent} due to invalid rewards data shape.\")\n",
    "\n",
    "        plt.xlabel(\"Steps\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.title(\"Time-to-Convergence\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_aggregate_metrics():\n",
    "        metrics = {\n",
    "            \"IQM\": [],\n",
    "            \"Median\": [],\n",
    "            \"Mean\": [],\n",
    "        }\n",
    "        agent_names = list(agent_eval_data.keys())\n",
    "\n",
    "        for agent, data in agent_eval_data.items():\n",
    "            rewards_matrix = np.array(data.rewards).reshape(len(SEEDS), N_EPISODES)\n",
    "\n",
    "            metrics[\"IQM\"].append(rliable.metrics.aggregate_iqm(rewards_matrix))\n",
    "            metrics[\"Median\"].append(rliable.metrics.aggregate_median(rewards_matrix))\n",
    "            metrics[\"Mean\"].append(rliable.metrics.aggregate_mean(rewards_matrix))\n",
    "\n",
    "        # Create subplots for each metric\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        for ax, (metric_name, values) in zip(axes, metrics.items()):\n",
    "            sns.barplot(\n",
    "                x=agent_names,\n",
    "                y=values,\n",
    "                ax=ax,\n",
    "                hue=agent_names,\n",
    "                palette=\"tab10\",\n",
    "                capsize=0.1,\n",
    "                legend=False,\n",
    "            )\n",
    "            ax.set_title(f\"{metric_name} Reward\")\n",
    "            ax.set_xlabel(\"Agent\")\n",
    "            ax.set_ylabel(\"Reward\")\n",
    "            ax.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "        plt.suptitle(\"Aggregate Evaluation Metrics (rliable)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_probability_of_improvement():\n",
    "        # Create a list of all agents (keys from agent_eval_data)\n",
    "        agents = list(agent_eval_data.keys())\n",
    "\n",
    "        # Dictionaries to hold the probability estimates and interval estimates\n",
    "        probability_estimates = {}\n",
    "        probability_interval_estimates = {}\n",
    "\n",
    "        # Compare each pair of agents using itertools.combinations\n",
    "        for agent1, agent2 in itertools.combinations(agents, 2):\n",
    "            # Get the rewards for each agent as lists\n",
    "            rewards1 = agent_eval_data[agent1].rewards\n",
    "            rewards2 = agent_eval_data[agent2].rewards\n",
    "\n",
    "            rewards1_reshaped = np.array(rewards1).reshape(len(SEEDS), N_EPISODES)\n",
    "            rewards2_reshaped = np.array(rewards2).reshape(len(SEEDS), N_EPISODES)\n",
    "\n",
    "            # Calculate the probability of improvement between the two agents\n",
    "            prob_improvement = rliable.metrics.probability_of_improvement(\n",
    "                rewards1_reshaped, rewards2_reshaped\n",
    "            )\n",
    "\n",
    "            # Calculate the confidence intervals (e.g., bootstrap method, here assuming it is available)\n",
    "            # If you have an existing method to calculate the intervals, apply it\n",
    "            # For simplicity, we use placeholders here\n",
    "            prob_interval = [0.0, 1.0]  # Replace with actual interval calculation\n",
    "\n",
    "            # Store the probability and interval estimates\n",
    "            pair = f\"{agent1},{agent2}\"\n",
    "            probability_estimates[pair] = prob_improvement\n",
    "            probability_interval_estimates[pair] = prob_interval\n",
    "\n",
    "        # Plot the probability of improvement using the rliable function\n",
    "        rliable.plot_utils.plot_probability_of_improvement(\n",
    "            probability_estimates,\n",
    "            probability_interval_estimates,\n",
    "            ax=None,\n",
    "            figsize=(8, 6),\n",
    "            colors=None,\n",
    "            color_palette=\"colorblind\",\n",
    "            alpha=0.75,\n",
    "            xlabel=\"P(X > Y)\",\n",
    "            left_ylabel=\"Algorithm X\",\n",
    "            right_ylabel=\"Algorithm Y\",\n",
    "        )\n",
    "\n",
    "        plt.title(\"Probability of Improvement between Algorithms\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_performance_profiles():\n",
    "        all_rewards = defaultdict(list)\n",
    "        for agent, data in agent_eval_data.items():\n",
    "            all_rewards[agent] = data.rewards\n",
    "        for agent, rewards in all_rewards.items():\n",
    "            sorted_rewards = np.sort(rewards)\n",
    "            plt.plot(\n",
    "                np.arange(len(sorted_rewards)),\n",
    "                sorted_rewards,\n",
    "                label=f\"{agent} Performance Profile\",\n",
    "            )\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.title(\"Performance Profiles\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_catastrophic_forgetting():\n",
    "        for agent, data in agent_eval_data.items():\n",
    "            forgetting = np.abs(\n",
    "                np.array(data.rewards) - np.mean(data.rewards)\n",
    "            )  # Simplified measure\n",
    "            plt.plot(data.episodes, forgetting, label=f\"{agent} Forgetting\")\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Catastrophic Forgetting (Deviation from Mean Reward)\")\n",
    "        plt.title(\"Catastrophic Forgetting\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_regret_analysis():\n",
    "        for agent, data in agent_eval_data.items():\n",
    "            regret = np.max(data.rewards) - np.array(data.rewards)\n",
    "            plt.plot(data.episodes, regret, label=f\"{agent} Regret\")\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Regret (Max - Current Reward)\")\n",
    "        plt.title(\"Regret Analysis\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_robustness_to_perturbations():\n",
    "        for agent, data in agent_eval_data.items():\n",
    "            noise = np.random.normal(\n",
    "                0, 0.1, size=len(data.rewards)\n",
    "            )  # Adding noise as perturbation\n",
    "            robustness = data.rewards + noise\n",
    "            plt.plot(data.episodes, robustness, label=f\"{agent} Robustness\")\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Reward (with Perturbations)\")\n",
    "        plt.title(\"Robustness to Perturbations\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_pareto_frontier():\n",
    "        all_rewards = defaultdict(list)\n",
    "        for agent, data in agent_eval_data.items():\n",
    "            all_rewards[agent] = data.rewards\n",
    "        for agent, rewards in all_rewards.items():\n",
    "            plt.scatter(\n",
    "                np.arange(len(rewards)), rewards, label=f\"{agent} Pareto Frontier\"\n",
    "            )\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.title(\"Pareto Frontier for Multi-Objective Optimization\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # Call all the plot functions\n",
    "    plot_learning_curve()\n",
    "    plot_training_stability()\n",
    "    plot_sample_efficiency()\n",
    "    plot_time_to_convergence()\n",
    "    plot_aggregate_metrics()\n",
    "    plot_probability_of_improvement()\n",
    "    plot_performance_profiles()\n",
    "    plot_catastrophic_forgetting()\n",
    "    plot_regret_analysis()\n",
    "    plot_robustness_to_perturbations()\n",
    "    plot_pareto_frontier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "\n",
    "with open(\"training_data_per_agent.pkl\", \"rb\") as f:\n",
    "    training_data_per_agent = pickle.load(f)\n",
    "\n",
    "with open(\"evaluation_data_per_agent.pkl\", \"rb\") as f:\n",
    "    evaluation_data_per_agent = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "plot_all_metrics(training_data_per_agent, evaluation_data_per_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
