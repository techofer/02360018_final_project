{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# electricity_market_player\n",
    "\n",
    "> This module training optimizing and evaluation of RL agent on the electricity market environment.\n",
    "using PPO with actions mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp electricity_market_player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from abc import ABC\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import rliable\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import yaml\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from electricity_market.electricity_market_env import ElectricityMarketEnv, EnvConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "N_EPISODES = 3\n",
    "N_TRAILS = 10\n",
    "SEEDS = [\n",
    "    100000,\n",
    "    101010,\n",
    "    101055,\n",
    "    111111,\n",
    "    123456,\n",
    "    157724,\n",
    "    200000,\n",
    "    216555,\n",
    "    217890,\n",
    "    222222,\n",
    "    224775,\n",
    "    234567,\n",
    "    253084,\n",
    "    285234,\n",
    "    312135,\n",
    "    314831,\n",
    "    333333,\n",
    "    345678,\n",
    "    406339,\n",
    "    444444,\n",
    "    471678,\n",
    "    526070,\n",
    "    526785,\n",
    "    555555,\n",
    "    562845,\n",
    "    666666,\n",
    "    701753,\n",
    "    730845,\n",
    "    755460,\n",
    "    761386,\n",
    "    777777,\n",
    "    789391,\n",
    "    877838,\n",
    "    878787,\n",
    "    888888,\n",
    "    900000,\n",
    "    993068,\n",
    "    979797,\n",
    "    987654,\n",
    "    989898,\n",
    "    999999,\n",
    "]\n",
    "\n",
    "\n",
    "training_data_per_agent = {}\n",
    "evaluation_data_per_agent = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingData:\n",
    "    steps: list[int]\n",
    "    episodes: list[int]\n",
    "    rewards: list[float]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvaluationData:\n",
    "    episodes: list[int]\n",
    "    rewards: list[float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def plot_all_metrics(\n",
    "    agent_train_data: dict[str, TrainingData],\n",
    "    agent_eval_data: dict[str, EvaluationData],\n",
    "):\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    def plot_learning_curve():\n",
    "        for agent, data in agent_train_data.items():\n",
    "            plt.plot(data.steps, data.rewards, label=f\"{agent} Learning Curve\")\n",
    "        plt.xlabel(\"Steps\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.title(\"Learning Curves\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_training_stability():\n",
    "        for agent, data in agent_train_data.items():\n",
    "            plt.plot(\n",
    "                data.steps,\n",
    "                np.cumsum(data.rewards) / (np.arange(len(data.steps)) + 1),\n",
    "                label=f\"{agent} Stability\",\n",
    "            )\n",
    "        plt.xlabel(\"Steps\")\n",
    "        plt.ylabel(\"Cumulative Average Reward\")\n",
    "        plt.title(\"Training Stability\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_sample_efficiency():\n",
    "        for agent, data in agent_train_data.items():\n",
    "            plt.plot(\n",
    "                data.steps,\n",
    "                np.cumsum(data.rewards) / (np.arange(len(data.steps)) + 1),\n",
    "                label=f\"{agent} Sample Efficiency\",\n",
    "            )\n",
    "        plt.xlabel(\"Steps\")\n",
    "        plt.ylabel(\"Cumulative Average Reward\")\n",
    "        plt.title(\"Sample Efficiency Curve\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_time_to_convergence():\n",
    "        for agent, data in agent_train_data.items():\n",
    "            # Ensure that data.rewards is a 1D array before using np.diff\n",
    "            if len(np.shape(data.rewards)) == 1:\n",
    "                converged_step = np.argmax(\n",
    "                    np.diff(data.rewards) < 0.01\n",
    "                )  # Example threshold for convergence\n",
    "                plt.axvline(\n",
    "                    x=data.steps[converged_step], label=f\"{agent} Time to Convergence\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"Skipping {agent} due to invalid rewards data shape.\")\n",
    "        plt.xlabel(\"Steps\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.title(\"Time-to-Convergence\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_aggregate_metrics():\n",
    "        for agent, data in agent_eval_data.items():\n",
    "            plt.plot(data.episodes, data.rewards, label=f\"{agent} Performance\")\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.title(\"Aggregate Evaluation Metrics\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_probability_of_improvement():\n",
    "        improvements = defaultdict(list)\n",
    "        for agent, data in agent_eval_data.items():\n",
    "            baseline_rewards = data.rewards[: len(data.rewards) // 2]\n",
    "            improvement = np.mean(data.rewards[len(data.rewards) // 2 :]) - np.mean(\n",
    "                baseline_rewards\n",
    "            )\n",
    "            improvements[agent] = improvement\n",
    "        plt.bar(improvements.keys(), improvements.values())\n",
    "        plt.xlabel(\"Agent\")\n",
    "        plt.ylabel(\"Average Improvement\")\n",
    "        plt.title(\"Probability of Improvement Between Algorithms\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_performance_profiles():\n",
    "        all_rewards = defaultdict(list)\n",
    "        for agent, data in agent_eval_data.items():\n",
    "            all_rewards[agent] = data.rewards\n",
    "        for agent, rewards in all_rewards.items():\n",
    "            sorted_rewards = np.sort(rewards)\n",
    "            plt.plot(\n",
    "                np.arange(len(sorted_rewards)),\n",
    "                sorted_rewards,\n",
    "                label=f\"{agent} Performance Profile\",\n",
    "            )\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.title(\"Performance Profiles\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_catastrophic_forgetting():\n",
    "        for agent, data in agent_eval_data.items():\n",
    "            forgetting = np.abs(\n",
    "                np.array(data.rewards) - np.mean(data.rewards)\n",
    "            )  # Simplified measure\n",
    "            plt.plot(data.episodes, forgetting, label=f\"{agent} Forgetting\")\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Catastrophic Forgetting (Deviation from Mean Reward)\")\n",
    "        plt.title(\"Catastrophic Forgetting\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_regret_analysis():\n",
    "        for agent, data in agent_eval_data.items():\n",
    "            regret = np.max(data.rewards) - np.array(data.rewards)\n",
    "            plt.plot(data.episodes, regret, label=f\"{agent} Regret\")\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Regret (Max - Current Reward)\")\n",
    "        plt.title(\"Regret Analysis\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_robustness_to_perturbations():\n",
    "        for agent, data in agent_eval_data.items():\n",
    "            noise = np.random.normal(\n",
    "                0, 0.1, size=len(data.rewards)\n",
    "            )  # Adding noise as perturbation\n",
    "            robustness = data.rewards + noise\n",
    "            plt.plot(data.episodes, robustness, label=f\"{agent} Robustness\")\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Reward (with Perturbations)\")\n",
    "        plt.title(\"Robustness to Perturbations\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_pareto_frontier():\n",
    "        all_rewards = defaultdict(list)\n",
    "        for agent, data in agent_eval_data.items():\n",
    "            all_rewards[agent] = data.rewards\n",
    "        for agent, rewards in all_rewards.items():\n",
    "            plt.scatter(\n",
    "                np.arange(len(rewards)), rewards, label=f\"{agent} Pareto Frontier\"\n",
    "            )\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.title(\"Pareto Frontier for Multi-Objective Optimization\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # Call all the plot functions\n",
    "    plot_learning_curve()\n",
    "    plot_training_stability()\n",
    "    plot_sample_efficiency()\n",
    "    plot_time_to_convergence()\n",
    "    plot_aggregate_metrics()\n",
    "    plot_probability_of_improvement()\n",
    "    plot_performance_profiles()\n",
    "    plot_catastrophic_forgetting()\n",
    "    plot_regret_analysis()\n",
    "    plot_robustness_to_perturbations()\n",
    "    plot_pareto_frontier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Agent(ABC):\n",
    "    def __init__(self, agent_name):\n",
    "        self.agent_name = agent_name\n",
    "\n",
    "    def train(self) -> TrainingData:\n",
    "        \"\"\"\n",
    "        Train the model, and return TrainingData\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def evaluate(self, render: bool = False) -> EvaluationData:\n",
    "        \"\"\"\n",
    "        Evaluate the model, and return EvaluationData\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save_model(self, model_path: Path):\n",
    "        \"\"\"\n",
    "        save the model\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def load_model(self, model_path: Path) -> None:\n",
    "        \"\"\"\n",
    "        load the model\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class MaskableAgent(Agent):\n",
    "    @staticmethod\n",
    "    def mask_fn(env):\n",
    "        \"\"\"\n",
    "        Placeholder mask function if needed.\n",
    "        \"\"\"\n",
    "        return env.action_masks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class MaskableRandomAgent(MaskableAgent):\n",
    "    def __init__(\n",
    "        self, env_config: EnvConfig | None = None, render_mode: str | None = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the agent and create the environment.\n",
    "        \"\"\"\n",
    "        super().__init__(agent_name=\"MaskableRandomAgent\")\n",
    "        self.env = ActionMasker(\n",
    "            ElectricityMarketEnv(env_config, render_mode=render_mode), self.mask_fn\n",
    "        )\n",
    "\n",
    "    def train(self) -> TrainingData:\n",
    "        \"\"\"\n",
    "        This agent does not train\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def evaluate(self, render: bool = False) -> EvaluationData:\n",
    "        \"\"\"\n",
    "        Evaluates the random agent by executing episodes in the environment.\n",
    "        \"\"\"\n",
    "        all_rewards = []\n",
    "\n",
    "        for seed in tqdm(SEEDS, desc=\"seeds\"):\n",
    "            # TODO: utilize sb3-contrib evaluate_policy\n",
    "            for _ in tqdm(range(N_EPISODES), desc=\"episodes\", leave=False):\n",
    "                episode_rewards = []\n",
    "\n",
    "                obs, _ = self.env.reset(seed=seed)\n",
    "                done = False\n",
    "                total_reward = 0.0\n",
    "\n",
    "                while not done:\n",
    "                    action_mask = self.env.action_masks()\n",
    "                    valid_actions = np.where(action_mask)[0]  # Get valid actions\n",
    "                    action = np.random.choice(\n",
    "                        valid_actions\n",
    "                    )  # Select random valid action\n",
    "\n",
    "                    obs, reward, done, truncated, _ = self.env.step(action)\n",
    "                    total_reward += reward\n",
    "\n",
    "                    if render:\n",
    "                        self.env.render()\n",
    "\n",
    "                    if truncated:\n",
    "                        break\n",
    "\n",
    "                episode_rewards.append(total_reward)\n",
    "\n",
    "                # Append episode results to the total list\n",
    "                all_rewards.append(total_reward)\n",
    "\n",
    "        return EvaluationData(\n",
    "            episodes=list(range(len(all_rewards))),\n",
    "            rewards=all_rewards,\n",
    "        )\n",
    "\n",
    "    def save_model(self, model_path: Path) -> None:\n",
    "        \"\"\"\n",
    "        No model to save for a random agent.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def load_model(self, model_path: Path) -> None:\n",
    "        \"\"\"\n",
    "        No model to load for a random agent.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class A2CAgent(Agent):\n",
    "    \"\"\"A2C Agent for the Electricity Market Environment.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, env_config: EnvConfig | None = None, render_mode: str | None = None\n",
    "    ):\n",
    "        super().__init__(agent_name=\"A2CAgent\")\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.env = Monitor(\n",
    "            ElectricityMarketEnv(env_config, render_mode=render_mode),\n",
    "        )\n",
    "        self.model = A2C(\n",
    "            \"MlpPolicy\",\n",
    "            self.env,\n",
    "            verbose=0,\n",
    "            tensorboard_log=f\"./{self.agent_name}_tensorboard/\",\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "    def train(self) -> TrainingData:\n",
    "        \"\"\"\n",
    "        Train the model, and return TrainingData.\n",
    "        \"\"\"\n",
    "        all_rewards = []\n",
    "        all_steps = []\n",
    "        total_steps = 0\n",
    "        checkpoint_callback = CheckpointCallback(save_freq=1000, save_path=\"./logs/\")\n",
    "\n",
    "        # Training loop\n",
    "        for seed in tqdm(SEEDS, desc=\"seeds\"):\n",
    "            for _ in tqdm(range(N_EPISODES), desc=\"Training episodes\"):\n",
    "                obs, _ = self.env.reset(seed=seed)\n",
    "                episode_rewards = []\n",
    "                steps = 0\n",
    "                done = False\n",
    "\n",
    "                while not done:\n",
    "                    # Convert observation to torch tensor\n",
    "                    obs_tensor = torch.tensor(obs, dtype=torch.float32).to(self.device)\n",
    "\n",
    "                    # Get action and action probability from the policy\n",
    "                    action, _ = self.model.predict(obs_tensor)\n",
    "\n",
    "                    obs, reward, done, truncated, _ = self.env.step(action)\n",
    "                    episode_rewards.append(reward)\n",
    "                    steps += 1\n",
    "\n",
    "                    # Perform learning step after each episode\n",
    "                    if done or truncated:\n",
    "                        self.model.learn(\n",
    "                            total_timesteps=steps, callback=checkpoint_callback\n",
    "                        )\n",
    "\n",
    "                all_rewards.append(np.sum(episode_rewards))\n",
    "                total_steps += steps\n",
    "                all_steps.append(total_steps)\n",
    "\n",
    "        return TrainingData(\n",
    "            steps=all_steps,\n",
    "            episodes=list(range(len(all_rewards))),\n",
    "            rewards=all_rewards,\n",
    "        )\n",
    "\n",
    "    def evaluate(self, render: bool = False) -> EvaluationData:\n",
    "        \"\"\"\n",
    "        Evaluate the model, and return EvaluationData.\n",
    "        \"\"\"\n",
    "        all_rewards = []\n",
    "\n",
    "        for seed in tqdm(SEEDS, desc=\"seeds\"):\n",
    "            # TODO: utilize sb3 evaluate_policy\n",
    "            for _ in tqdm(range(N_EPISODES), desc=\"Evaluation episodes\"):\n",
    "                obs, _ = self.env.reset(seed=seed)\n",
    "                episode_rewards = []\n",
    "                done = False\n",
    "\n",
    "                while not done:\n",
    "                    # Convert observation to torch tensor\n",
    "                    obs_tensor = torch.tensor(obs, dtype=torch.float32).to(self.device)\n",
    "\n",
    "                    action, _ = self.model.predict(obs_tensor, deterministic=True)\n",
    "                    obs, reward, done, truncated, _ = self.env.step(action)\n",
    "                    episode_rewards.append(reward)\n",
    "\n",
    "                    if render:\n",
    "                        self.env.render()\n",
    "\n",
    "                    if truncated:\n",
    "                        break\n",
    "\n",
    "                all_rewards.append(np.sum(episode_rewards))\n",
    "\n",
    "        return EvaluationData(\n",
    "            episodes=list(range(len(all_rewards))),\n",
    "            rewards=all_rewards,\n",
    "        )\n",
    "\n",
    "    def save_model(self, model_path: Path) -> None:\n",
    "        self.model.save(str(model_path))\n",
    "\n",
    "    def load_model(self, model_path: Path) -> None:\n",
    "        self.model.load(str(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class MaskablePPOAgent(MaskableAgent):\n",
    "    \"\"\"Maskable PPO Agent for the Electricity Market Environment.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, env_config: EnvConfig | None = None, render_mode: str | None = None\n",
    "    ):\n",
    "        super().__init__(agent_name=\"MaskablePPOAgent\")\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.env_config = env_config or EnvConfig()\n",
    "        self.env = Monitor(\n",
    "            ActionMasker(\n",
    "                ElectricityMarketEnv(env_config, render_mode=render_mode),\n",
    "                self.mask_fn,\n",
    "            )\n",
    "        )\n",
    "        self.optimized_hyperparameters = {}\n",
    "        self.model = MaskablePPO(\n",
    "            MaskableActorCriticPolicy,\n",
    "            self.env,\n",
    "            **self.optimized_hyperparameters,\n",
    "            verbose=0,\n",
    "            tensorboard_log=f\"./{self.agent_name}_tensorboard/\",\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "    def train(self) -> TrainingData:\n",
    "        \"\"\"\n",
    "        Train the model, and return TrainingData.\n",
    "        \"\"\"\n",
    "        all_rewards = []\n",
    "        all_steps = []\n",
    "        total_steps = 0\n",
    "        checkpoint_callback = CheckpointCallback(save_freq=1000, save_path=\"./logs/\")\n",
    "\n",
    "        # Training loop\n",
    "        for seed in tqdm(SEEDS, desc=\"seeds\"):\n",
    "            for _ in tqdm(range(N_EPISODES), desc=\"Training episodes\"):\n",
    "                obs, _ = self.env.reset(seed=seed)\n",
    "                episode_rewards = []\n",
    "                steps = 0\n",
    "                done = False\n",
    "\n",
    "                while not done:\n",
    "                    # Convert observation to torch tensor\n",
    "                    obs_tensor = torch.tensor(obs, dtype=torch.float32).to(self.device)\n",
    "\n",
    "                    action, _ = self.model.predict(obs_tensor)\n",
    "\n",
    "                    obs, reward, done, truncated, _ = self.env.step(action)\n",
    "                    episode_rewards.append(reward)\n",
    "                    steps += 1\n",
    "\n",
    "                    # Perform learning step after each episode\n",
    "                    if done or truncated:\n",
    "                        self.model.learn(\n",
    "                            total_timesteps=steps,\n",
    "                            use_masking=True,\n",
    "                            callback=checkpoint_callback,\n",
    "                        )\n",
    "\n",
    "                all_rewards.append(np.sum(episode_rewards))\n",
    "                total_steps += steps\n",
    "                all_steps.append(total_steps)\n",
    "\n",
    "        return TrainingData(\n",
    "            steps=all_steps,\n",
    "            episodes=list(range(len(all_rewards))),\n",
    "            rewards=all_rewards,\n",
    "        )\n",
    "\n",
    "    def evaluate(self, render: bool = False) -> EvaluationData:\n",
    "        \"\"\"\n",
    "        Evaluate the model, and return EvaluationData.\n",
    "        \"\"\"\n",
    "        all_rewards = []\n",
    "\n",
    "        for seed in tqdm(SEEDS, desc=\"seeds\"):\n",
    "            for _ in tqdm(range(N_EPISODES), desc=\"Evaluation episodes\"):\n",
    "                obs, _ = self.env.reset(seed=seed)\n",
    "                episode_rewards = []\n",
    "                done = False\n",
    "\n",
    "                while not done:\n",
    "                    # Convert observation to torch tensor\n",
    "                    obs_tensor = torch.tensor(obs, dtype=torch.float32).to(self.device)\n",
    "\n",
    "                    action, _ = self.model.predict(obs_tensor, deterministic=True)\n",
    "                    obs, reward, done, truncated, _ = self.env.step(action)\n",
    "                    episode_rewards.append(reward)\n",
    "\n",
    "                    if render:\n",
    "                        self.env.render()\n",
    "\n",
    "                    if truncated:\n",
    "                        break\n",
    "\n",
    "                all_rewards.append(np.sum(episode_rewards))\n",
    "\n",
    "        return EvaluationData(\n",
    "            episodes=list(range(len(all_rewards))),\n",
    "            rewards=all_rewards,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def collect_episodes_rewards(model, env) -> list[float]:\n",
    "        \"\"\"\n",
    "        Collect rewards from evaluating the agent for a given number of episodes.\n",
    "        \"\"\"\n",
    "        rewards = []\n",
    "        for seed in tqdm(SEEDS, desc=\"seeds\"):\n",
    "            for _ in tqdm(range(N_EPISODES), desc=\"Collect episodes\"):\n",
    "                obs, _ = env.reset(seed=seed)\n",
    "                episode_rewards = 0\n",
    "                done = False\n",
    "\n",
    "                while not done:\n",
    "                    action, _ = model.predict(obs, deterministic=True)\n",
    "                    obs, reward, done, truncated, _ = env.step(action)\n",
    "                    episode_rewards += reward\n",
    "                rewards.append(episode_rewards)\n",
    "\n",
    "        return rewards\n",
    "\n",
    "    def optimize(self) -> None:\n",
    "        \"\"\"\n",
    "        Optimize the agent with hyperparameters and return TrainingData.\n",
    "        \"\"\"\n",
    "\n",
    "        def objective(trial):\n",
    "            # Hyperparameter search space\n",
    "            learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "            n_steps = trial.suggest_int(\"n_steps\", 32, 1024, log=True)\n",
    "            batch_size = trial.suggest_int(\"batch_size\", 16, 256, log=True)\n",
    "            gamma = trial.suggest_float(\"gamma\", 0.9, 0.9999)\n",
    "            gae_lambda = trial.suggest_float(\"gae_lambda\", 0.8, 1.0)\n",
    "            ent_coef = trial.suggest_float(\"ent_coef\", 0.0, 0.02)\n",
    "            vf_coef = trial.suggest_float(\"vf_coef\", 0.1, 1.0)\n",
    "            clip_range = trial.suggest_float(\"clip_range\", 0.1, 0.3)\n",
    "            max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.1, 1.0)\n",
    "\n",
    "            # Set up environment and model with the sampled hyperparameters\n",
    "            env = Monitor(\n",
    "                ActionMasker(\n",
    "                    ElectricityMarketEnv(self.env_config, render_mode=\"human\"),\n",
    "                    self.mask_fn,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            model = MaskablePPO(\n",
    "                MaskableActorCriticPolicy,\n",
    "                env,\n",
    "                learning_rate=learning_rate,\n",
    "                n_steps=n_steps,\n",
    "                batch_size=batch_size,\n",
    "                gamma=gamma,\n",
    "                gae_lambda=gae_lambda,\n",
    "                ent_coef=ent_coef,\n",
    "                vf_coef=vf_coef,\n",
    "                clip_range=clip_range,\n",
    "                max_grad_norm=max_grad_norm,\n",
    "                verbose=0,\n",
    "                tensorboard_log=f\"./{self.agent_name}_tensorboard/\",\n",
    "                device=self.device,\n",
    "            )\n",
    "\n",
    "            # Train and evaluate\n",
    "            model.learn(\n",
    "                total_timesteps=self.env_config.max_timestep,\n",
    "                use_masking=True,\n",
    "                reset_num_timesteps=False,\n",
    "            )\n",
    "\n",
    "            # Collect rewards for evaluation\n",
    "            episode_rewards = self.collect_episodes_rewards(model, env)\n",
    "\n",
    "            # Return the mean reward as the optimization goal\n",
    "            return np.mean(episode_rewards)\n",
    "\n",
    "        # Create a study to optimize the objective function\n",
    "        study = optuna.create_study(\n",
    "            study_name=self.agent_name,\n",
    "            storage=\"sqlite:///optuna_study.db\",\n",
    "            load_if_exists=True,\n",
    "            direction=\"maximize\",\n",
    "            pruner=optuna.pruners.HyperbandPruner(),\n",
    "            sampler=optuna.samplers.TPESampler(),\n",
    "        )\n",
    "\n",
    "        # Optimize using the defined objective function\n",
    "        study.optimize(objective, n_trials=N_TRAILS, n_jobs=-1, show_progress_bar=True)\n",
    "\n",
    "        # Save the best hyperparameters\n",
    "        self.optimized_hyperparameters = study.best_params\n",
    "\n",
    "        # Re-train the model with the optimized hyperparameters\n",
    "        self.model = MaskablePPO(\n",
    "            MaskableActorCriticPolicy,\n",
    "            self.env,\n",
    "            **self.optimized_hyperparameters,\n",
    "            verbose=0,\n",
    "            tensorboard_log=f\"./{self.agent_name}_tensorboard/\",\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "    def export_hyperparameters(self, filename: str):\n",
    "        \"\"\"\n",
    "        Export optimized hyperparameters to a YAML file.\n",
    "        \"\"\"\n",
    "        with open(filename, \"w\") as file:\n",
    "            yaml.dump(self.optimized_hyperparameters, file)\n",
    "\n",
    "    def save_model(self, model_path: Path) -> None:\n",
    "        self.model.save(str(model_path))\n",
    "\n",
    "    def load_model(self, model_path: Path) -> None:\n",
    "        self.model.load(str(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Evaluation MaskableRandom on ElectricityMarketEnv\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e724263cd364e65990b66d8da310941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "seeds:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aadbec6d440d4a86896aebc4a249f20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5bf2386a9c54558b8492acf2813644e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9679fb42701f4c30bd164dd7b6c69ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6649399a77114571a0365aa594ccffcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f45ec6eac04b0c90022876a1b40f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8cb487b1a04246b84cb6cd542158e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc471c86d2f4553afc238145a958c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b623ba5a0e4b23bdd9bd5cb338d283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b741a0611fee4642950609b4cf0a3b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb60d69579eb419abdf1aeb2d1fb426e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f40dc6f26014e2db30449e4750500da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4d989c31d343ccb66d8b8135877fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e95bf2a88cf4b578ee439605ef70070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b49822938fc4449b062fa0d2c756111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25493380fc5945c5af092b44fe763952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d558cc6e51d9485191cba78e89cc8163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344921086bcb4dd9ad4148c5fea4f4f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314604eaf43b49c6802277935e150c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43feb9c9237644979ebda5365c3d8848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6a932b5fac4feeb27319f6b6eebb9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d2fe48ef98489c9bcf329f4657d163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8d56fcee0648709afc0ddc5f7b8ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed05c30e66514510b73c782f9caa5cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b9c61312714e158c491d8aebe145fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7368ee0d016b48159c34e8c138b8d441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac4719cc64d140f4aba19475dc00fb98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a953a6feb1d4f5ca84386920fc07701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af9444337184f819602c1283b15e515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69b53e1883e4546b79e5674be431143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77fbf03dce6147298317e5883dd74884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f02052962bc49528e68cea1b6166d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ca17df9b68495fbcd5bda23088923f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce87d9a4f80489dbbcad8ee0c1037fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f068e4977e4e3b98f229959919e580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc57b9b77b541379ab39d18fb964b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b495c237ab499a9e091644d08decd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8cb5196ccbf4faea0c747be80d59bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda7ef7890d4459b8579f309c124fcc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b7601669d734f45a9be19a1f183d9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6501bd158fff4c88b2fa34384f5e0e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f777eada474849ae95a25fd1fac0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | export\n",
    "maskable_random_agent = MaskableRandomAgent(render_mode=\"human\")\n",
    "\n",
    "evaluation_data_per_agent[maskable_random_agent.agent_name] = (\n",
    "    maskable_random_agent.evaluate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Evaluation A2C on ElectricityMarketEnv\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256a7a4edb104b66bdebaa8e688ab963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "seeds:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33990e53e07a4e8ab02d0ee1a07a9c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8080e1c91a7d4435be1db180a1c4d728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "179eda68ec214fedb1d5947b2f404842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | export\n",
    "a2c_agent = A2CAgent(render_mode=\"human\")\n",
    "\n",
    "training_data_per_agent[a2c_agent.agent_name] = a2c_agent.train()\n",
    "\n",
    "a2c_agent.save_model(f\"{a2c_agent.agent_name}.model\")\n",
    "\n",
    "evaluation_data_per_agent[a2c_agent.agent_name] = a2c_agent.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Evaluation MaskablePPO with default hyperparameters on ElectricityMarketEnv"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "maskable_ppo_agent = MaskablePPOAgent(render_mode=\"human\")\n",
    "\n",
    "training_data_per_agent[maskable_ppo_agent.agent_name] = maskable_ppo_agent.train()\n",
    "\n",
    "maskable_ppo_agent.save_model(f\"{maskable_ppo_agent.agent_name}.model\")\n",
    "\n",
    "evaluation_data_per_agent[maskable_ppo_agent.agent_name] = maskable_ppo_agent.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Evaluation MaskablePPO with optimized hyperparameters on ElectricityMarketEnv"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "optimized_maskable_ppo_agent = MaskablePPOAgent(render_mode=\"human\")\n",
    "optimized_maskable_ppo_agent.agent_name = \"OptimizedMaskablePPOAgent\"\n",
    "optimized_maskable_ppo_agent.optimize()\n",
    "\n",
    "optimized_maskable_ppo_agent.export_hyperparameters(\n",
    "    f\"{optimized_maskable_ppo_agent.agent_name}.yaml\"\n",
    ")\n",
    "\n",
    "\n",
    "training_data_per_agent[optimized_maskable_ppo_agent.agent_name] = (\n",
    "    optimized_maskable_ppo_agent.train()\n",
    ")\n",
    "\n",
    "optimized_maskable_ppo_agent.save_model(\n",
    "    f\"{optimized_maskable_ppo_agent.agent_name}.model\"\n",
    ")\n",
    "\n",
    "evaluation_data_per_agent[optimized_maskable_ppo_agent.agent_name] = (\n",
    "    optimized_maskable_ppo_agent.evaluate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Adding expert knowledge to the masking function making learning more efficient"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def is_action_safe(self, action: int) -> bool:\n",
    "    charge_amount = self._charge_amount(action)\n",
    "    target_state_of_charge = self._current_state_of_charge + charge_amount\n",
    "    low, high = self._battery_safe_range\n",
    "    return high > target_state_of_charge > low\n",
    "\n",
    "\n",
    "def expert_knowledge_action_masks(self) -> np.ndarray:\n",
    "    mask = np.array(\n",
    "        [\n",
    "            self._is_action_valid(action) and self.is_action_safe(action)\n",
    "            for action in range(self.action_space.n)\n",
    "        ],\n",
    "        dtype=bool,\n",
    "    )\n",
    "    if not np.any(mask):  # If all actions are invalid, force one to be valid\n",
    "        mask[len(mask) // 2] = True\n",
    "    return mask\n",
    "\n",
    "\n",
    "# Dynamically overriding action_masks to ElectricityMarketEnv\n",
    "setattr(ElectricityMarketEnv, \"action_masks\", expert_knowledge_action_masks)\n",
    "# Dynamically overriding injection is_action_safe to ElectricityMarketEnv\n",
    "setattr(ElectricityMarketEnv, \"is_action_safe\", is_action_safe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Evaluation MaskableRandomAgent with Expert Knowledge on ElectricityMarketEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "expert_maskable_random_agent = MaskableRandomAgent(render_mode=\"human\")\n",
    "expert_maskable_random_agent.agent_name = \"ExpertMaskableRandomAgent\"\n",
    "\n",
    "evaluation_data_per_agent[expert_maskable_random_agent.agent_name] = (\n",
    "    expert_maskable_random_agent.evaluate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Evaluation MaskablePPO with default hyperparameters and Expert Knowledge on ElectricityMarketEnv"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "expert_maskable_ppo_agent = MaskablePPOAgent(render_mode=\"human\")\n",
    "expert_maskable_ppo_agent.agent_name = \"ExpertMaskablePPOAgent\"\n",
    "\n",
    "training_data_per_agent[expert_maskable_ppo_agent.agent_name] = (\n",
    "    expert_maskable_ppo_agent.train()\n",
    ")\n",
    "\n",
    "expert_maskable_ppo_agent.save_model(f\"{expert_maskable_ppo_agent.agent_name}.model\")\n",
    "\n",
    "evaluation_data_per_agent[expert_maskable_ppo_agent.agent_name] = (\n",
    "    expert_maskable_ppo_agent.evaluate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Evaluation MaskablePPO with optimized hyperparameters and Expert Knowledge on ElectricityMarketEnv\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "optimized_expert_maskable_ppo_agent = MaskablePPOAgent(render_mode=\"human\")\n",
    "optimized_expert_maskable_ppo_agent.agent_name = \"OptimizedExpertMaskablePPOAgent\"\n",
    "optimized_expert_maskable_ppo_agent.optimize()\n",
    "\n",
    "optimized_expert_maskable_ppo_agent.export_hyperparameters(\n",
    "    f\"{optimized_expert_maskable_ppo_agent.agent_name}.yaml\"\n",
    ")\n",
    "\n",
    "training_data_per_agent[optimized_expert_maskable_ppo_agent.agent_name] = (\n",
    "    optimized_expert_maskable_ppo_agent.train()\n",
    ")\n",
    "\n",
    "optimized_expert_maskable_ppo_agent.save_model(\n",
    "    f\"{optimized_expert_maskable_ppo_agent.agent_name}.model\"\n",
    ")\n",
    "\n",
    "evaluation_data_per_agent[optimized_expert_maskable_ppo_agent.agent_name] = (\n",
    "    optimized_expert_maskable_ppo_agent.evaluate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "plot_all_metrics(training_data_per_agent, evaluation_data_per_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
