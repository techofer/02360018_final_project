{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# electricity_market_player\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#| default_exp electricity_market_player"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import optuna\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from sb3_contrib.common.maskable.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "from electricity_market.electricity_market_env import ElectricityMarketEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Evaluation\n",
      "mean_reward:15043267.07 +/- 66641.68\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "def mask_fn(env):\n",
    "    return env.action_masks()\n",
    "\n",
    "\n",
    "env_config = {}\n",
    "\n",
    "env = DummyVecEnv([lambda: Monitor(ActionMasker(ElectricityMarketEnv(env_config), mask_fn))])\n",
    "model = MaskablePPO(MaskableActorCriticPolicy, env, verbose=0, seed=123456)\n",
    "print(\"Training\")\n",
    "model.learn(total_timesteps=10_000, use_masking=True)\n",
    "print(\"Evaluation\")\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 00:31:02,471] A new study created in memory with name: no-name-12667c86-4a68-45eb-ad42-fc168dc62ae8\n",
      "[I 2025-02-09 00:32:07,232] Trial 0 finished with value: 20043323.9037663 and parameters: {'learning_rate': 0.01879331175017602, 'n_steps': 601, 'batch_size': 66}. Best is trial 0 with value: 20043323.9037663.\n",
      "[I 2025-02-09 00:33:06,966] Trial 1 finished with value: 14970182.111348 and parameters: {'learning_rate': 1.3617257402106057e-05, 'n_steps': 180, 'batch_size': 142}. Best is trial 0 with value: 20043323.9037663.\n",
      "[I 2025-02-09 00:34:29,021] Trial 2 finished with value: 17070417.523969598 and parameters: {'learning_rate': 0.001832540881813732, 'n_steps': 28, 'batch_size': 29}. Best is trial 0 with value: 20043323.9037663.\n",
      "[I 2025-02-09 00:35:24,500] Trial 3 finished with value: 17010377.707562797 and parameters: {'learning_rate': 0.003259978663445662, 'n_steps': 154, 'batch_size': 235}. Best is trial 0 with value: 20043323.9037663.\n",
      "[I 2025-02-09 00:36:27,582] Trial 4 finished with value: 19163328.254677698 and parameters: {'learning_rate': 0.006072784441966549, 'n_steps': 1888, 'batch_size': 65}. Best is trial 0 with value: 20043323.9037663.\n",
      "[I 2025-02-09 00:38:13,920] Trial 5 finished with value: 13567943.982719202 and parameters: {'learning_rate': 1.0057869933223445e-05, 'n_steps': 33, 'batch_size': 26}. Best is trial 0 with value: 20043323.9037663.\n",
      "[I 2025-02-09 00:39:26,575] Trial 6 finished with value: 13643068.303709399 and parameters: {'learning_rate': 0.00037620342952550287, 'n_steps': 113, 'batch_size': 55}. Best is trial 0 with value: 20043323.9037663.\n",
      "[I 2025-02-09 00:40:56,059] Trial 7 finished with value: 11074337.8386987 and parameters: {'learning_rate': 0.00014280138905124583, 'n_steps': 24, 'batch_size': 73}. Best is trial 0 with value: 20043323.9037663.\n",
      "[I 2025-02-09 00:41:59,759] Trial 8 finished with value: 16032073.077694202 and parameters: {'learning_rate': 1.1415553914446124e-05, 'n_steps': 69, 'batch_size': 140}. Best is trial 0 with value: 20043323.9037663.\n",
      "[I 2025-02-09 00:43:40,904] Trial 9 finished with value: 19010762.426997 and parameters: {'learning_rate': 4.500644300260506e-05, 'n_steps': 124, 'batch_size': 18}. Best is trial 0 with value: 20043323.9037663.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: FrozenTrial(number=0, state=1, values=[20043323.9037663], datetime_start=datetime.datetime(2025, 2, 9, 0, 31, 2, 472342), datetime_complete=datetime.datetime(2025, 2, 9, 0, 32, 7, 232559), params={'learning_rate': 0.01879331175017602, 'n_steps': 601, 'batch_size': 66}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': FloatDistribution(high=0.1, log=True, low=1e-05, step=None), 'n_steps': IntDistribution(high=2048, log=True, low=16, step=1), 'batch_size': IntDistribution(high=256, log=True, low=16, step=1)}, trial_id=0, value=None)\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "def optimize_agent(trial):\n",
    "    # Define hyperparameters to optimize\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    n_steps = trial.suggest_int('n_steps', 16, 2048, log=True)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 256, log=True)\n",
    "\n",
    "    # Create environment\n",
    "    env = DummyVecEnv([lambda: Monitor(ActionMasker(ElectricityMarketEnv(env_config), mask_fn))])\n",
    "\n",
    "\n",
    "    # Create the MaskablePPO model with suggested hyperparameters\n",
    "    model = MaskablePPO(\n",
    "        MaskableActorCriticPolicy,\n",
    "        env,\n",
    "        learning_rate=learning_rate,\n",
    "        n_steps=n_steps,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0,\n",
    "        seed=123456\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.learn(total_timesteps=int(1e5))\n",
    "\n",
    "    # Evaluate the model using action masking\n",
    "    mean_reward, _ = evaluate_policy(\n",
    "        model, env, n_eval_episodes=10, deterministic=True\n",
    "    )\n",
    "\n",
    "    return mean_reward\n",
    "\n",
    "# Set up Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(optimize_agent, n_trials=10)\n",
    "\n",
    "# Print best trial\n",
    "print(\"Best trial:\", study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Evaluation\n",
      "mean_reward:16380868.28 +/- 58081.60\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "model = MaskablePPO(MaskableActorCriticPolicy, env, verbose=0, seed=123456, **study.best_trial.params)\n",
    "\n",
    "print(\"Training\")\n",
    "model.learn(total_timesteps=10_000, use_masking=True)\n",
    "print(\"Evaluation\")\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
