{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# electricity_market_player\n",
    "\n",
    "> This module training optimizing and evaluation of RL agent on the electricity market environment.\n",
    "using PPO with actions mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp electricity_market_player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from abc import ABC\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from rliable import library as rly\n",
    "from rliable import metrics, plot_utils\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.maskable.evaluation import (\n",
    "    evaluate_policy as maskable_evaluate_policy,\n",
    ")\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from scipy import stats\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.base_class import BaseAlgorithm\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.evaluation import (\n",
    "    evaluate_policy as non_maskable_evaluate_policy,\n",
    ")\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from tqdm import tqdm\n",
    "\n",
    "from electricity_market.electricity_market_env import ElectricityMarketEnv, EnvConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "TOTAL_TIMESTEPS = 10_000\n",
    "N_EPISODES = 10\n",
    "N_TRAILS = 10\n",
    "N_JOBS = 7\n",
    "seeds = [123456]  # , 234567] #, 345678, 456789, 567890]\n",
    "number_of_frames = 5\n",
    "frame_size = TOTAL_TIMESTEPS // number_of_frames\n",
    "frames = np.array(list(range(frame_size, TOTAL_TIMESTEPS + 1, frame_size)), dtype=int)\n",
    "\n",
    "env_config = EnvConfig(max_timestep=TOTAL_TIMESTEPS)\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Agent(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def mask_fn(cls, env: ElectricityMarketEnv) -> np.ndarray:\n",
    "        return env.action_masks()\n",
    "\n",
    "    @classmethod\n",
    "    def collect_episodes_rewards(\n",
    "        cls,\n",
    "        model: BaseAlgorithm | None,\n",
    "        env: ElectricityMarketEnv,\n",
    "        n_episodes: int = N_EPISODES,\n",
    "        deterministic: bool = True,\n",
    "        render: bool = False,\n",
    "        seed: int | None = None,\n",
    "    ) -> list[float]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @classmethod\n",
    "    def evaluate_policy(\n",
    "        cls,\n",
    "        hyperparameters: dict | None = None,\n",
    "        n_episodes: int = N_EPISODES,\n",
    "        render: bool = False,\n",
    "    ) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @classmethod\n",
    "    def optimize_agent(cls, trial, n_episodes: int = N_EPISODES):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class MaskableRandomAgent(Agent):\n",
    "    @classmethod\n",
    "    def collect_episodes_rewards(\n",
    "        cls,\n",
    "        model: BaseAlgorithm | None,  # Unused for a random agent\n",
    "        env: ElectricityMarketEnv,\n",
    "        n_episodes: int = N_EPISODES,\n",
    "        deterministic: bool = True,\n",
    "        render: bool = False,\n",
    "        seed: int | None = None,\n",
    "    ) -> list[float]:\n",
    "        episode_rewards = []\n",
    "\n",
    "        for _ in range(n_episodes):\n",
    "            obs, _ = env.reset(seed=seed)\n",
    "            done = False\n",
    "            total_reward = 0.0\n",
    "\n",
    "            while not done:\n",
    "                action_mask = env.action_masks()\n",
    "                valid_actions = np.where(action_mask)[0]  # Get valid actions\n",
    "                action = np.random.choice(valid_actions)  # Select random valid action\n",
    "\n",
    "                obs, reward, done, truncated, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "                if truncated:\n",
    "                    break\n",
    "\n",
    "            episode_rewards.append(total_reward)\n",
    "\n",
    "        return episode_rewards\n",
    "\n",
    "    @classmethod\n",
    "    def evaluate_policy(\n",
    "        cls,\n",
    "        hyperparameters: dict | None = None,\n",
    "        n_episodes: int = N_EPISODES,\n",
    "        render: bool = False,\n",
    "    ) -> np.ndarray:\n",
    "        global seeds, frames, env_config\n",
    "\n",
    "        all_rewards = []\n",
    "\n",
    "        for seed in tqdm(seeds, desc=\"seeds\"):\n",
    "            env = ActionMasker(\n",
    "                ElectricityMarketEnv(env_config, render_mode=\"human\"), cls.mask_fn\n",
    "            )\n",
    "\n",
    "            seed_rewards = []\n",
    "\n",
    "            for _ in tqdm(frames, desc=\"frames\"):\n",
    "                rewards = cls.collect_episodes_rewards(\n",
    "                    None, env, n_episodes, deterministic=True, render=render, seed=seed\n",
    "                )\n",
    "                seed_rewards.append(rewards)\n",
    "            seed_rewards = np.array(seed_rewards)\n",
    "            all_rewards.append(seed_rewards)\n",
    "\n",
    "        all_rewards = np.array(\n",
    "            all_rewards\n",
    "        )  # Shape: (num_seeds, num_checkpoints, num_episodes)\n",
    "        print(\n",
    "            \"\\nCollected Rewards (shape: seeds x checkpoints x episodes):\\n\",\n",
    "            all_rewards,\n",
    "        )\n",
    "\n",
    "        return all_rewards\n",
    "\n",
    "    @classmethod\n",
    "    def optimize_agent(cls, trial, n_episodes: int = N_EPISODES):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class A2CAgent(Agent):\n",
    "    @classmethod\n",
    "    def collect_episodes_rewards(\n",
    "        cls,\n",
    "        model: A2C,\n",
    "        env: ElectricityMarketEnv,\n",
    "        n_episodes: int = N_EPISODES,\n",
    "        deterministic: bool = True,\n",
    "        render: bool = False,\n",
    "        seed: int | None = None,\n",
    "    ) -> list[float]:\n",
    "        env.reset(seed=seed)\n",
    "        episode_rewards, _ = non_maskable_evaluate_policy(\n",
    "            model,\n",
    "            env,\n",
    "            deterministic=deterministic,\n",
    "            return_episode_rewards=True,\n",
    "            n_eval_episodes=n_episodes,\n",
    "            render=render,\n",
    "        )\n",
    "        return episode_rewards\n",
    "\n",
    "    @classmethod\n",
    "    def evaluate_policy(\n",
    "        cls,\n",
    "        hyperparameters: dict | None = None,\n",
    "        n_episodes: int = N_EPISODES,\n",
    "        render: bool = False,\n",
    "    ) -> np.ndarray:\n",
    "        global seeds, frames, env_config\n",
    "\n",
    "        if hyperparameters is None:\n",
    "            hyperparameters = {}\n",
    "        all_rewards = []\n",
    "\n",
    "        for seed in tqdm(seeds, desc=\"seeds\"):\n",
    "            env = Monitor(\n",
    "                ElectricityMarketEnv(env_config, render_mode=\"human\"),\n",
    "            )\n",
    "\n",
    "            model = A2C(\n",
    "                \"MlpPolicy\",\n",
    "                env,\n",
    "                verbose=0,\n",
    "                seed=seed,\n",
    "                **hyperparameters,\n",
    "                tensorboard_log=\"./a2c_tensorboard/\",\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            )\n",
    "            checkpoint_callback = CheckpointCallback(\n",
    "                save_freq=10000, save_path=\"./logs/\"\n",
    "            )\n",
    "            seed_rewards = []\n",
    "\n",
    "            for frame in tqdm(frames, desc=\"frames\", leave=False):\n",
    "                model.learn(\n",
    "                    total_timesteps=frame,\n",
    "                    reset_num_timesteps=False,\n",
    "                    callback=checkpoint_callback,\n",
    "                )\n",
    "                rewards = cls.collect_episodes_rewards(\n",
    "                    model,\n",
    "                    env,\n",
    "                    n_episodes=n_episodes,\n",
    "                    deterministic=True,\n",
    "                    render=render,\n",
    "                    seed=seed,\n",
    "                )\n",
    "                seed_rewards.append(rewards)\n",
    "\n",
    "            seed_rewards = np.array(\n",
    "                seed_rewards\n",
    "            )  # Shape: (num_checkpoints, num_episodes)\n",
    "            all_rewards.append(seed_rewards)\n",
    "\n",
    "        all_rewards = np.array(\n",
    "            all_rewards\n",
    "        )  # Shape: (num_seeds, num_checkpoints, num_episodes)\n",
    "        print(\n",
    "            \"\\nCollected Rewards (shape: seeds x checkpoints x episodes):\\n\",\n",
    "            all_rewards,\n",
    "        )\n",
    "        return all_rewards\n",
    "\n",
    "    @classmethod\n",
    "    def optimize_agent(cls, trial, n_episodes: int = N_EPISODES):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class MaskablePPOAgent(Agent):\n",
    "    @classmethod\n",
    "    def collect_episodes_rewards(\n",
    "        cls,\n",
    "        model: MaskablePPO,\n",
    "        env: ElectricityMarketEnv,\n",
    "        n_episodes: int = N_EPISODES,\n",
    "        deterministic: bool = True,\n",
    "        render: bool = False,\n",
    "        seed: int | None = None,\n",
    "    ) -> list[float]:\n",
    "        env.reset(seed=seed)\n",
    "        episode_rewards, _ = maskable_evaluate_policy(\n",
    "            model,\n",
    "            env,\n",
    "            deterministic=deterministic,\n",
    "            use_masking=True,\n",
    "            return_episode_rewards=True,\n",
    "            n_eval_episodes=n_episodes,\n",
    "            render=render,\n",
    "        )\n",
    "        return episode_rewards\n",
    "\n",
    "    @classmethod\n",
    "    def evaluate_policy(\n",
    "        cls,\n",
    "        hyperparameters: dict | None = None,\n",
    "        n_episodes: int = N_EPISODES,\n",
    "        render: bool = False,\n",
    "    ) -> np.ndarray:\n",
    "        global seeds, frames, env_config\n",
    "\n",
    "        if hyperparameters is None:\n",
    "            hyperparameters = {}\n",
    "        all_rewards = []\n",
    "\n",
    "        for seed in tqdm(seeds, desc=\"seeds\"):\n",
    "            env = Monitor(\n",
    "                ActionMasker(\n",
    "                    ElectricityMarketEnv(env_config, render_mode=\"human\"),\n",
    "                    cls.mask_fn,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            model = MaskablePPO(\n",
    "                MaskableActorCriticPolicy,\n",
    "                env,\n",
    "                verbose=0,\n",
    "                seed=seed,\n",
    "                **hyperparameters,\n",
    "                tensorboard_log=\"./maskable_ppo_tensorboard/\",\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            )\n",
    "            checkpoint_callback = CheckpointCallback(\n",
    "                save_freq=10000, save_path=\"./logs/\"\n",
    "            )\n",
    "            seed_rewards = []\n",
    "\n",
    "            for frame in tqdm(frames, desc=\"frames\", leave=False):\n",
    "                model.learn(\n",
    "                    total_timesteps=frame,\n",
    "                    use_masking=True,\n",
    "                    reset_num_timesteps=False,\n",
    "                    callback=checkpoint_callback,\n",
    "                )\n",
    "                rewards = cls.collect_episodes_rewards(\n",
    "                    model,\n",
    "                    env,\n",
    "                    n_episodes=n_episodes,\n",
    "                    deterministic=True,\n",
    "                    render=render,\n",
    "                    seed=seed,\n",
    "                )\n",
    "                seed_rewards.append(rewards)\n",
    "\n",
    "            seed_rewards = np.array(\n",
    "                seed_rewards\n",
    "            )  # Shape: (num_checkpoints, num_episodes)\n",
    "            all_rewards.append(seed_rewards)\n",
    "\n",
    "        all_rewards = np.array(\n",
    "            all_rewards\n",
    "        )  # Shape: (num_seeds, num_checkpoints, num_episodes)\n",
    "        print(\n",
    "            \"\\nCollected Rewards (shape: seeds x checkpoints x episodes):\\n\",\n",
    "            all_rewards,\n",
    "        )\n",
    "        return all_rewards\n",
    "\n",
    "    @classmethod\n",
    "    def optimize_agent(cls, trial, n_episodes: int = N_EPISODES):\n",
    "        global seeds\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "        n_steps = trial.suggest_int(\"n_steps\", 32, 1024, log=True)\n",
    "        batch_size = trial.suggest_int(\"batch_size\", 16, 256, log=True)\n",
    "        gamma = trial.suggest_float(\"gamma\", 0.9, 0.9999)\n",
    "        gae_lambda = trial.suggest_float(\"gae_lambda\", 0.8, 1.0)\n",
    "        ent_coef = trial.suggest_float(\"ent_coef\", 0.0, 0.02)\n",
    "        vf_coef = trial.suggest_float(\"vf_coef\", 0.1, 1.0)\n",
    "        clip_range = trial.suggest_float(\"clip_range\", 0.1, 0.3)\n",
    "        max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.1, 1.0)\n",
    "\n",
    "        trial_seed_rewards = []\n",
    "\n",
    "        for seed in tqdm(seeds, desc=\"seeds\"):\n",
    "            env = Monitor(\n",
    "                ActionMasker(\n",
    "                    ElectricityMarketEnv(env_config, render_mode=\"human\"),\n",
    "                    maskable_ppo_agent.mask_fn,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            model = MaskablePPO(\n",
    "                MaskableActorCriticPolicy,\n",
    "                env,\n",
    "                learning_rate=learning_rate,\n",
    "                n_steps=n_steps,\n",
    "                batch_size=batch_size,\n",
    "                gamma=gamma,\n",
    "                gae_lambda=gae_lambda,\n",
    "                ent_coef=ent_coef,\n",
    "                vf_coef=vf_coef,\n",
    "                clip_range=clip_range,\n",
    "                max_grad_norm=max_grad_norm,\n",
    "                verbose=0,\n",
    "                seed=seed,\n",
    "            )\n",
    "\n",
    "            model.learn(total_timesteps=TOTAL_TIMESTEPS, use_masking=True)\n",
    "            episode_rewards = cls.collect_episodes_rewards(\n",
    "                model,\n",
    "                env,\n",
    "                n_episodes=n_episodes,\n",
    "                deterministic=True,\n",
    "                render=False,\n",
    "                seed=seed,\n",
    "            )\n",
    "\n",
    "            seed_avg_reward = np.mean(episode_rewards)\n",
    "            trial_seed_rewards.append(seed_avg_reward)\n",
    "        aggregated_performance = stats.trim_mean(\n",
    "            trial_seed_rewards, proportiontocut=0.25\n",
    "        )\n",
    "\n",
    "        return aggregated_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# Function to compute aggregated metrics for each algorithm\n",
    "\n",
    "\n",
    "def aggregate_func(x):\n",
    "    return np.array(\n",
    "        [\n",
    "            metrics.aggregate_median(x),\n",
    "            metrics.aggregate_iqm(x),\n",
    "            metrics.aggregate_mean(x),\n",
    "        ],\n",
    "        dtype=np.float64,\n",
    "    )\n",
    "\n",
    "\n",
    "# Aggregate results across seeds and episodes for each algorithm\n",
    "\n",
    "\n",
    "def aggregate_over_checkpoints(evaluation_results):\n",
    "    aggregated_results = {}\n",
    "    for algorithm, results in evaluation_results.items():\n",
    "        # results.shape is (num_seeds, num_checkpoints, num_episodes)\n",
    "        # We aggregate across seeds and episodes for each checkpoint\n",
    "        agg_results = np.array(\n",
    "            [aggregate_func(results[:, i, :]) for i in range(results.shape[1])]\n",
    "        )\n",
    "        aggregated_results[algorithm] = agg_results\n",
    "    return aggregated_results\n",
    "\n",
    "\n",
    "# Function to plot aggregate metrics (Median, IQM, Mean)\n",
    "\n",
    "\n",
    "def plot_aggregate_metrics(aggregated_results, algorithms):\n",
    "    aggregate_scores, aggregate_score_cis = rly.get_interval_estimates(\n",
    "        aggregated_results, aggregate_func, reps=50000\n",
    "    )\n",
    "\n",
    "    metric_names = [\"Median\", \"IQM\", \"Mean\"]\n",
    "    fig, axes = plot_utils.plot_interval_estimates(\n",
    "        aggregate_scores,\n",
    "        aggregate_score_cis,\n",
    "        metric_names=metric_names,\n",
    "        algorithms=algorithms,\n",
    "        xlabel=\"Reward\",\n",
    "    )\n",
    "    fig.set_size_inches(10, 5)\n",
    "    plt.suptitle(\n",
    "        \"Aggregate Metrics with 95% Stratified Bootstrap CIs\", y=1.05, fontsize=16\n",
    "    )\n",
    "    plt.xticks(rotation=45, fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Function to plot the probability of improvement between two algorithms\n",
    "\n",
    "\n",
    "def plot_probability_of_improvement(evaluation_results, algorithms):\n",
    "    for alg1, alg2 in combinations(algorithms, 2):\n",
    "        algorithm_pairs = {\n",
    "            f\"{alg1},{alg2}\": (evaluation_results[alg1], evaluation_results[alg2])\n",
    "        }\n",
    "\n",
    "        average_probabilities, average_prob_cis = rly.get_interval_estimates(\n",
    "            algorithm_pairs, metrics.probability_of_improvement, reps=2000\n",
    "        )\n",
    "\n",
    "        plot_utils.plot_probability_of_improvement(\n",
    "            average_probabilities, average_prob_cis\n",
    "        )\n",
    "        plt.title(f\"Probability of Improvement: {alg1} vs {alg2}\", pad=20)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Function to plot the sample efficiency curve\n",
    "\n",
    "\n",
    "def plot_sample_efficiency_curve(evaluation_results, frames):\n",
    "    global number_of_frames\n",
    "    sample_efficiency_dict = {\n",
    "        alg: results[:, :, :]\n",
    "        for alg, results in evaluation_results.items()\n",
    "        if len(results.shape) == 3\n",
    "    }\n",
    "\n",
    "    iqm_func = lambda scores: np.array(\n",
    "        [\n",
    "            metrics.aggregate_iqm(scores[:, :, frame])\n",
    "            for frame in range(number_of_frames)\n",
    "        ]\n",
    "    )\n",
    "    iqm_scores, iqm_cis = rly.get_interval_estimates(\n",
    "        sample_efficiency_dict, iqm_func, reps=50000\n",
    "    )\n",
    "\n",
    "    plot_utils.plot_sample_efficiency_curve(\n",
    "        frames=frames + 1,  # Adjust frames if necessary\n",
    "        point_estimates=iqm_scores,\n",
    "        interval_estimates=iqm_cis,\n",
    "        algorithms=sample_efficiency_dict.keys(),\n",
    "        xlabel=\"Number of Frames\",\n",
    "        ylabel=\"IQM Reward\",\n",
    "    )\n",
    "    plt.title(\"Sample Efficiency Curve\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Function to plot performance profiles (linear and non-linear scaling)\n",
    "\n",
    "\n",
    "def plot_performance_profiles(evaluation_results, algorithms):\n",
    "    thresholds = np.linspace(0.0, 8.0, 81)\n",
    "    score_distributions, score_distributions_cis = rly.create_performance_profile(\n",
    "        evaluation_results, thresholds\n",
    "    )\n",
    "\n",
    "    # Plot performance profiles with linear scale\n",
    "    fig, ax = plt.subplots(ncols=1, figsize=(7, 5))\n",
    "    plot_utils.plot_performance_profiles(\n",
    "        score_distributions,\n",
    "        thresholds,\n",
    "        performance_profile_cis=score_distributions_cis,\n",
    "        colors=dict(zip(algorithms, sns.color_palette(\"colorblind\"))),\n",
    "        xlabel=r\"Normalized Score $(\\tau)$\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    plt.title(\"Performance Profiles (Linear Scale)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot performance profiles with non-linear scaling\n",
    "    thresholds = np.logspace(-1, 0, num=50)\n",
    "    fig, ax = plt.subplots(ncols=1, figsize=(7, 5))\n",
    "    plot_utils.plot_performance_profiles(\n",
    "        score_distributions,\n",
    "        thresholds,\n",
    "        performance_profile_cis=score_distributions_cis,\n",
    "        use_non_linear_scaling=True,\n",
    "        colors=dict(zip(algorithms, sns.color_palette(\"colorblind\"))),\n",
    "        xlabel=r\"Normalized Score $(\\tau)$\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    plt.title(\"Performance Profiles (Non-Linear Scaling)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_learning_curves(evaluation_results, algorithms):\n",
    "    \"\"\"Plots learning curves over training timesteps for multiple algorithms.\"\"\"\n",
    "    global frames\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Convert frames (checkpoints) to cumulative timesteps\n",
    "    total_timesteps = frames  # X-axis: total timesteps\n",
    "\n",
    "    for algorithm in algorithms:\n",
    "        rewards = evaluation_results[algorithm]  # Shape: (seeds, checkpoints, episodes)\n",
    "\n",
    "        # Mean reward per checkpoint (average over seeds and episodes)\n",
    "        mean_rewards_per_checkpoint = np.mean(rewards, axis=(0, 2))\n",
    "        std_rewards_per_checkpoint = np.std(rewards, axis=(0, 2))\n",
    "\n",
    "        plt.plot(\n",
    "            total_timesteps,\n",
    "            mean_rewards_per_checkpoint,\n",
    "            label=algorithm,\n",
    "            marker=\"o\",\n",
    "            linestyle=\"-\",\n",
    "        )\n",
    "        plt.fill_between(\n",
    "            total_timesteps,\n",
    "            mean_rewards_per_checkpoint - std_rewards_per_checkpoint,\n",
    "            mean_rewards_per_checkpoint + std_rewards_per_checkpoint,\n",
    "            alpha=0.2,\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"Total Timesteps\", fontsize=14)\n",
    "    plt.ylabel(\"Mean Episode Reward\", fontsize=14)\n",
    "    plt.title(\"Learning Curves Over Training Timesteps\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    plt.legend(loc=\"best\", fontsize=12, title=\"Algorithms\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    plt.tight_layout(pad=2.0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Main function to call all the individual plot functions\n",
    "\n",
    "\n",
    "def plot_evaluation_results(evaluation_results: dict) -> None:\n",
    "    global frames\n",
    "    algorithms = list(evaluation_results.keys())\n",
    "\n",
    "    # Plot Learning Curves\n",
    "    plot_learning_curves(evaluation_results, algorithms)\n",
    "\n",
    "    # Aggregate the results across seeds and checkpoints\n",
    "    aggregated_results = aggregate_over_checkpoints(evaluation_results)\n",
    "\n",
    "    # Plot aggregate metrics\n",
    "    plot_aggregate_metrics(aggregated_results, algorithms)\n",
    "\n",
    "    # Plot Probability of Improvement\n",
    "    plot_probability_of_improvement(evaluation_results, algorithms)\n",
    "\n",
    "    # Plot Sample Efficiency Curve\n",
    "    plot_sample_efficiency_curve(evaluation_results, frames)\n",
    "\n",
    "    # Plot Performance Profiles\n",
    "    plot_performance_profiles(evaluation_results, algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "a2c_agent = A2CAgent()\n",
    "maskable_random_agent = MaskableRandomAgent()\n",
    "maskable_ppo_agent = MaskablePPOAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "results[\"A2CAgent\"] = a2c_agent.evaluate_policy(\n",
    "    hyperparameters=None, n_episodes=N_EPISODES, render=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "plot_evaluation_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Evaluation MaskableRandom on ElectricityMarketEnv\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "results[\"MaskableRandomAgent\"] = maskable_random_agent.evaluate_policy(\n",
    "    hyperparameters=None, n_episodes=N_EPISODES, render=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "plot_evaluation_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Evaluation MaskablePPO with default hyperparameters on ElectricityMarketEnv"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "results[\"MaskablePPOAgent_Baseline\"] = maskable_ppo_agent.evaluate_policy(\n",
    "    hyperparameters=None, n_episodes=N_EPISODES, render=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "plot_evaluation_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Hypertuning MaskablePPO with default hyperparameters on ElectricityMarketEnv"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(maskable_ppo_agent.optimize_agent, n_trials=N_TRAILS, n_jobs=N_JOBS)\n",
    "\n",
    "print(\"Best trial:\", study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Evaluation MaskablePPO with optimized hyperparameters on ElectricityMarketEnv"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "results[\"MaskablePPOAgent_Optimized\"] = maskable_ppo_agent.evaluate_policy(\n",
    "    hyperparameters=study.best_trial.params, n_episodes=N_EPISODES, render=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "plot_evaluation_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
