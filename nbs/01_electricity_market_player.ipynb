{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# electricity_market_player\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#| default_exp electricity_market_player"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import seaborn as sns\n",
    "\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from sb3_contrib.common.maskable.evaluation import evaluate_policy\n",
    "from scipy import stats\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from rliable import metrics, plot_utils, library as rly\n",
    "\n",
    "from electricity_market.electricity_market_env import ElectricityMarketEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_episode_rewards(model, env, n_episodes=10, deterministic=True):\n",
    "    episode_rewards, _ = evaluate_policy(\n",
    "        model, env, deterministic=deterministic, use_masking=True,\n",
    "        return_episode_rewards=True, n_eval_episodes=n_episodes\n",
    "    )\n",
    "    return episode_rewards\n",
    "\n",
    "\n",
    "def mask_fn(env):\n",
    "    return env.action_masks()\n",
    "\n",
    "\n",
    "seeds = [123456, 234567, 345678, 456789, 567890]\n",
    "env_config = {}\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_maskable_ppo_on_environment(hyperparameters=None):\n",
    "    global seeds\n",
    "\n",
    "    if hyperparameters is None:\n",
    "        hyperparameters = {}\n",
    "\n",
    "    frames = np.array(list(range(1000, 10001, 1000)), dtype=int)\n",
    "\n",
    "    all_rewards = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        print(f\"\\nRunning experiment with seed {seed}...\")\n",
    "        env = DummyVecEnv([\n",
    "            lambda: Monitor(ActionMasker(ElectricityMarketEnv(env_config), mask_fn))\n",
    "        ])\n",
    "\n",
    "        model = MaskablePPO(\n",
    "            MaskableActorCriticPolicy,\n",
    "            env,\n",
    "            verbose=0,\n",
    "            seed=seed,\n",
    "            **hyperparameters\n",
    "        )\n",
    "\n",
    "        seed_rewards = []\n",
    "\n",
    "        for frame in frames:\n",
    "            model.learn(total_timesteps=frame, use_masking=True, reset_num_timesteps=False)\n",
    "            rewards = collect_episode_rewards(model, env, n_episodes=10, deterministic=True)\n",
    "            seed_rewards.append(rewards)\n",
    "\n",
    "        seed_rewards = np.array(seed_rewards)  # Shape: (num_checkpoints, num_episodes)\n",
    "        all_rewards.append(seed_rewards)\n",
    "\n",
    "    all_rewards = np.array(all_rewards)  # Shape: (num_seeds, num_checkpoints, num_episodes)\n",
    "    print(\"\\nCollected Rewards (shape: seeds x checkpoints x episodes):\\n\", all_rewards)\n",
    "    return all_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluation_results(evaluation_results):\n",
    "    # Extract algorithm names (which are actually keys in the dictionary)\n",
    "    algorithms = list(evaluation_results.keys())\n",
    "\n",
    "    # Function to compute aggregate metrics (median, IQM, mean) for each checkpoint and seed\n",
    "    def aggregate_func(x):\n",
    "        return np.array([\n",
    "            metrics.aggregate_median(x),\n",
    "            metrics.aggregate_iqm(x),\n",
    "            metrics.aggregate_mean(x),\n",
    "        ])\n",
    "\n",
    "    # For each algorithm, we need to apply aggregate_func to the data (which has the shape (num_seeds, num_checkpoints, num_episodes))\n",
    "    def aggregate_over_checkpoints(evaluation_results):\n",
    "        aggregated_results = {}\n",
    "        for algorithm, results in evaluation_results.items():\n",
    "            # results.shape is (num_seeds, num_checkpoints, num_episodes)\n",
    "            # We aggregate across seeds and episodes for each checkpoint\n",
    "            agg_results = np.array([aggregate_func(results[:, i, :]) for i in range(results.shape[1])])\n",
    "            aggregated_results[algorithm] = agg_results\n",
    "        return aggregated_results\n",
    "\n",
    "    # Aggregate results across seeds and episodes\n",
    "    aggregated_results = aggregate_over_checkpoints(evaluation_results)\n",
    "\n",
    "    # Use rly to compute interval estimates\n",
    "    aggregate_scores, aggregate_score_cis = rly.get_interval_estimates(\n",
    "        aggregated_results, aggregate_func, reps=50000\n",
    "    )\n",
    "\n",
    "    # Plot aggregate metrics (Median, IQM, Mean)\n",
    "    metric_names = ['Median', 'IQM', 'Mean']\n",
    "    fig, axes = plot_utils.plot_interval_estimates(\n",
    "        aggregate_scores,\n",
    "        aggregate_score_cis,\n",
    "        metric_names=metric_names,\n",
    "        algorithms=algorithms,\n",
    "        xlabel='Reward'\n",
    "    )\n",
    "    plt.suptitle(\"Aggregate Metrics with 95% Stratified Bootstrap CIs\")\n",
    "    plt.show()\n",
    "\n",
    "    # =============================================================================\n",
    "    # 2. Probability of Improvement (if comparing two algorithms)\n",
    "    # =============================================================================\n",
    "    if len(algorithms) == 2:\n",
    "        alg1, alg2 = algorithms\n",
    "        algorithm_pairs = {f\"{alg1},{alg2}\": (evaluation_results[alg1], evaluation_results[alg2])}\n",
    "\n",
    "        average_probabilities, average_prob_cis = rly.get_interval_estimates(\n",
    "            algorithm_pairs, metrics.probability_of_improvement, reps=2000\n",
    "        )\n",
    "\n",
    "        plot_utils.plot_probability_of_improvement(average_probabilities, average_prob_cis)\n",
    "        plt.title(f\"Probability of Improvement: {alg1} vs {alg2}\")\n",
    "        plt.show()\n",
    "\n",
    "    # =============================================================================\n",
    "    # 3. Sample Efficiency Curve (using frames as defined in the evaluation function)\n",
    "    # =============================================================================\n",
    "    frames = np.array(list(range(1000, 10001, 1000)), dtype=int)\n",
    "    sample_efficiency_dict = {\n",
    "        alg: results[:, 1:, :]  # We want to remove the first checkpoint as it's usually 0\n",
    "        for alg, results in evaluation_results.items() if len(results.shape) == 3\n",
    "    }\n",
    "\n",
    "    # Define the IQM function\n",
    "    iqm_func = lambda scores: np.array([metrics.aggregate_iqm(scores[:, :, frame]) for frame in range(scores.shape[2])])\n",
    "\n",
    "    # Compute IQM scores and confidence intervals using rly\n",
    "    iqm_scores, iqm_cis = rly.get_interval_estimates(sample_efficiency_dict, iqm_func, reps=50000)\n",
    "    print(f\"Frames: {frames}\")\n",
    "    print(f\"IQM Scores: {iqm_scores}\")\n",
    "    print(f\"IQM CIs: {iqm_cis}\")\n",
    "    # Plot the sample efficiency curve\n",
    "    plot_utils.plot_sample_efficiency_curve(\n",
    "        frames=frames + 1,  # Adjust frames if necessary\n",
    "        point_estimates=iqm_scores,\n",
    "        interval_estimates=iqm_cis,\n",
    "        algorithms=sample_efficiency_dict.keys(),\n",
    "        xlabel='Number of Frames (in thousands)',\n",
    "        ylabel='IQM Reward'\n",
    "    )\n",
    "    plt.title(\"Sample Efficiency Curve\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # =============================================================================\n",
    "    # 4. Performance Profiles (linear and non-linear scaling)\n",
    "    # =============================================================================\n",
    "    thresholds = np.linspace(0.0, 8.0, 81)  # Define the thresholds\n",
    "    score_distributions, score_distributions_cis = rly.create_performance_profile(\n",
    "        evaluation_results, thresholds\n",
    "    )\n",
    "\n",
    "    # Plot performance profiles with linear scale\n",
    "    fig, ax = plt.subplots(ncols=1, figsize=(7, 5))\n",
    "    plot_utils.plot_performance_profiles(\n",
    "        score_distributions,\n",
    "        thresholds,\n",
    "        performance_profile_cis=score_distributions_cis,\n",
    "        colors=dict(zip(algorithms, sns.color_palette('colorblind'))),\n",
    "        xlabel=r'Normalized Score $(\\tau)$',\n",
    "        ax=ax\n",
    "    )\n",
    "    plt.title(\"Performance Profiles (Linear Scale)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot performance profiles with non-linear scaling\n",
    "    fig, ax = plt.subplots(ncols=1, figsize=(7, 5))\n",
    "    plot_utils.plot_performance_profiles(\n",
    "        score_distributions,\n",
    "        thresholds,\n",
    "        performance_profile_cis=score_distributions_cis,\n",
    "        use_non_linear_scaling=True,\n",
    "        xticks=[0.0, 0.5, 1.0, 2.0, 4.0, 8.0],\n",
    "        colors=dict(zip(algorithms, sns.color_palette('colorblind'))),\n",
    "        xlabel=r'Normalized Score $(\\tau)$',\n",
    "        ax=ax\n",
    "    )\n",
    "    plt.title(\"Performance Profiles (Non-Linear Scaling)\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_maskable_ppo_agent(trial):\n",
    "    global seeds\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    n_steps = trial.suggest_int('n_steps', 16, 2048, log=True)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 256, log=True)\n",
    "\n",
    "    trial_seed_rewards = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        env = DummyVecEnv([\n",
    "            lambda: Monitor(ActionMasker(ElectricityMarketEnv(env_config), mask_fn))\n",
    "        ])\n",
    "\n",
    "        model = MaskablePPO(\n",
    "            MaskableActorCriticPolicy,\n",
    "            env,\n",
    "            learning_rate=learning_rate,\n",
    "            n_steps=n_steps,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "        model.learn(total_timesteps=int(1e5), use_masking=True)\n",
    "        episode_rewards = collect_episode_rewards(model, env, n_episodes=10, deterministic=True)\n",
    "\n",
    "        seed_avg_reward = np.mean(episode_rewards)\n",
    "        trial_seed_rewards.append(seed_avg_reward)\n",
    "    aggregated_performance = stats.trim_mean(trial_seed_rewards, proportiontocut=0.25)\n",
    "\n",
    "    return aggregated_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Evaluation MaskablePPO with default hyperparameters on ElectricityMarketEnv"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment with seed 123456...\n"
     ]
    }
   ],
   "source": "results[\"MaskablePPO_Baseline\"] = evaluate_maskable_ppo_on_environment(hyperparameters=None)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Hypertuning MaskablePPO with default hyperparameters on ElectricityMarketEnv"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(optimize_maskable_ppo_agent, n_trials=10)\n",
    "\n",
    "print(\"Best trial:\", study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Evaluation MaskablePPO with optimized hyperparameters on ElectricityMarketEnv"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "results[\"MaskablePPO_Optimized\"] = evaluate_maskable_ppo_on_environment(hyperparameters=study.best_trial.params)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "plot_evaluation_results(results)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
