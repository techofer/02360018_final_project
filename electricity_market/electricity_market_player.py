"""Fill in a module description here"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_electricity_market_player.ipynb.

# %% auto 0
__all__ = ['env_config', 'env', 'model', 'mean_reward', 'std_reward', 'study', 'mask_fn', 'optimize_agent']

# %% ../nbs/01_electricity_market_player.ipynb 3
import optuna
from sb3_contrib import MaskablePPO
from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy
from sb3_contrib.common.wrappers import ActionMasker
from sb3_contrib.common.maskable.evaluation import evaluate_policy
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv

from .electricity_market_env import ElectricityMarketEnv


# %% ../nbs/01_electricity_market_player.ipynb 4
def mask_fn(env):
    return env.action_masks()


env_config = {}

env = DummyVecEnv([lambda: Monitor(ActionMasker(ElectricityMarketEnv(env_config), mask_fn))])
model = MaskablePPO(MaskableActorCriticPolicy, env, verbose=0, seed=123456)
print("Training")
model.learn(total_timesteps=10_000, use_masking=True)
print("Evaluation")
mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)
print(f"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}")

# %% ../nbs/01_electricity_market_player.ipynb 5
def optimize_agent(trial):
    # Define hyperparameters to optimize
    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)
    n_steps = trial.suggest_int('n_steps', 16, 2048, log=True)
    batch_size = trial.suggest_int('batch_size', 16, 256, log=True)

    # Create environment
    env = DummyVecEnv([lambda: Monitor(ActionMasker(ElectricityMarketEnv(env_config), mask_fn))])


    # Create the MaskablePPO model with suggested hyperparameters
    model = MaskablePPO(
        MaskableActorCriticPolicy,
        env,
        learning_rate=learning_rate,
        n_steps=n_steps,
        batch_size=batch_size,
        verbose=0,
        seed=123456
    )

    # Train the model
    model.learn(total_timesteps=int(1e5))

    # Evaluate the model using action masking
    mean_reward, _ = evaluate_policy(
        model, env, n_eval_episodes=10, deterministic=True
    )

    return mean_reward

# Set up Optuna study
study = optuna.create_study(direction="maximize")
study.optimize(optimize_agent, n_trials=10)

# Print best trial
print("Best trial:", study.best_trial)

# %% ../nbs/01_electricity_market_player.ipynb 6
model = MaskablePPO(MaskableActorCriticPolicy, env, verbose=0, seed=123456, **study.best_trial.params)

print("Training")
model.learn(total_timesteps=10_000, use_masking=True)
print("Evaluation")
mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)
print(f"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}")
