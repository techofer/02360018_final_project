# player


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

------------------------------------------------------------------------

<a
href="https://github.com/techofer/02360018_final_project/blob/main/electricity_market/player.py#L199"
target="_blank" style="float:right; font-size:smaller">source</a>

### MaskableModelAgent

>  MaskableModelAgent (name, env, env_config, model, device)

*Helper class that provides a standard way to create an ABC using
inheritance.*

<details open class="code-fold">
<summary>Exported source</summary>

``` python
class Agent(ABC):
    def __init__(self, name, env, device):
        self.name = name
        self.device = device
        self.env = env

    def evaluate(self, render: bool = False) -> EvaluationData:
        """
        Evaluate the model, and return EvaluationData.
        """
        all_rewards = []

        for seed in tqdm(EVALUATE_SEEDS, desc="seeds"):
            obs, _ = self.env.reset(seed=seed)
            episode_rewards = []
            done = False

            while not done:
                obs_tensor = torch.tensor(obs, dtype=torch.float64).to(self.device)

                action = self.choose_action(obs_tensor)
                obs, reward, done, truncated, _ = self.env.step(action)
                episode_rewards.append(reward)

                if render:
                    self.env.render()

                if truncated:
                    break

            all_rewards.append(np.sum(episode_rewards))

        return EvaluationData(
            episodes=list(range(len(all_rewards))),
            rewards=all_rewards,
        )

    def choose_action(self, obs_tensor):
        raise NotImplementedError


class ModelAgent(Agent):
    def __init__(self, name, env, env_config, model, device):
        super().__init__(name, device=device, env=env)
        self.model = model
        self.env_config = env_config

    def train(self) -> None:
        """
        Train the model
        """
        checkpoint_callback = CheckpointCallback(save_freq=1000, save_path="../logs/")

        for seed in tqdm(TRAIN_SEEDS, desc="seeds"):
            for _ in tqdm(range(N_TRAIN_EPISODES), desc="Training episodes"):
                self.env.reset(seed=seed)
                self.model.learn(
                    total_timesteps=self.env_config.max_timestep,
                    callback=checkpoint_callback,
                    reset_num_timesteps=False,
                    tb_log_name=self.name,
                )

    def choose_action(self, obs_tensor):
        action, _ = self.model.predict(obs_tensor, deterministic=True)
        return action

    def save_model(self, model_path: Path) -> None:
        self.model.save(str(model_path))

    def load_model(self, model_path: Path) -> None:
        self.model = self.model.load(str(model_path), env=self.env)


class MaskableAgent(Agent):
    @staticmethod
    def mask_fn(env):
        """
        Placeholder mask function if needed.
        """
        return env.unwrapped.action_masks()


class MaskableModelAgent(MaskableAgent, ModelAgent):
    def choose_action(self, obs_tensor):
        action, _ = self.model.predict(
            obs_tensor,
            action_masks=MaskableAgent.mask_fn(self.env),
            deterministic=True,
        )
```

</details>

------------------------------------------------------------------------

<a
href="https://github.com/techofer/02360018_final_project/blob/main/electricity_market/player.py#L190"
target="_blank" style="float:right; font-size:smaller">source</a>

### MaskableAgent

>  MaskableAgent (name, env, device)

*Helper class that provides a standard way to create an ABC using
inheritance.*

------------------------------------------------------------------------

<a
href="https://github.com/techofer/02360018_final_project/blob/main/electricity_market/player.py#L157"
target="_blank" style="float:right; font-size:smaller">source</a>

### ModelAgent

>  ModelAgent (name, env, env_config, model, device)

*Helper class that provides a standard way to create an ABC using
inheritance.*

------------------------------------------------------------------------

<a
href="https://github.com/techofer/02360018_final_project/blob/main/electricity_market/player.py#L116"
target="_blank" style="float:right; font-size:smaller">source</a>

### Agent

>  Agent (name, env, device)

*Helper class that provides a standard way to create an ABC using
inheritance.*

------------------------------------------------------------------------

<a
href="https://github.com/techofer/02360018_final_project/blob/main/electricity_market/player.py#L208"
target="_blank" style="float:right; font-size:smaller">source</a>

### MaskableRandomAgent

>  MaskableRandomAgent
>                           (env_config:electricity_market.env.EnvConfig|None=No
>                           ne, render_mode:str|None=None,
>                           name:str='MaskableRandomAgent')

*Helper class that provides a standard way to create an ABC using
inheritance.*

<details open class="code-fold">
<summary>Exported source</summary>

``` python
class MaskableRandomAgent(MaskableAgent):
    def __init__(
        self,
        env_config: EnvConfig | None = None,
        render_mode: str | None = None,
        name: str = "MaskableRandomAgent",
    ):
        """
        Initialize the agent and create the environment.
        """
        device = "cuda" if torch.cuda.is_available() else "cpu"
        env = ActionMasker(
            ElectricityMarketEnv(env_config, render_mode=render_mode), self.mask_fn
        )
        super().__init__(name, device=device, env=env)

    def choose_action(self, obs_tensor):
        action_mask = self.env.action_masks()
        valid_actions = np.where(action_mask)[0]
        action = np.random.choice(valid_actions)

        return action
```

</details>

------------------------------------------------------------------------

<a
href="https://github.com/techofer/02360018_final_project/blob/main/electricity_market/player.py#L232"
target="_blank" style="float:right; font-size:smaller">source</a>

### A2CAgent

>  A2CAgent (env_config:electricity_market.env.EnvConfig|None=None,
>                render_mode:str|None=None, name:str='A2CAgent')

*A2C Agent for the Electricity Market Environment.*

<details open class="code-fold">
<summary>Exported source</summary>

``` python
class A2CAgent(ModelAgent):
    """A2C Agent for the Electricity Market Environment."""

    def __init__(
        self,
        env_config: EnvConfig | None = None,
        render_mode: str | None = None,
        name: str = "A2CAgent",
    ):
        device = "cuda" if torch.cuda.is_available() else "cpu"
        env = Monitor(
            ElectricityMarketEnv(env_config, render_mode=render_mode),
        )
        model = A2C(
            "MlpPolicy",
            env,
            verbose=0,
            tensorboard_log=f"./tensorboard/",
            device=device,
        )
        super().__init__(
            name=name, env=env, model=model, device=device, env_config=env_config
        )
```

</details>

------------------------------------------------------------------------

<a
href="https://github.com/techofer/02360018_final_project/blob/main/electricity_market/player.py#L257"
target="_blank" style="float:right; font-size:smaller">source</a>

### MaskablePPOAgent

>  MaskablePPOAgent (env_config:electricity_market.env.EnvConfig|None=None,
>                        render_mode:str|None=None, name:str='MaskablePPOAgent')

*Maskable PPO Agent for the Electricity Market Environment.*

<details open class="code-fold">
<summary>Exported source</summary>

``` python
class MaskablePPOAgent(ModelAgent, MaskableAgent):
    """Maskable PPO Agent for the Electricity Market Environment."""

    def __init__(
        self,
        env_config: EnvConfig | None = None,
        render_mode: str | None = None,
        name: str = "MaskablePPOAgent",
    ):
        env = Monitor(
            ActionMasker(
                ElectricityMarketEnv(env_config, render_mode=render_mode),
                self.mask_fn,
            )
        )
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model = MaskablePPO(
            MaskableActorCriticPolicy,
            env,
            verbose=0,
            tensorboard_log=f"./tensorboard/",
            device=device,
        )
        super().__init__(
            name=name, env=env, model=model, device=device, env_config=env_config
        )
        self.optimized_hyperparameters = {}
        self.env_config = env_config or EnvConfig()

    def choose_action(self, obs_tensor):
        action, _ = self.model.predict(
            obs_tensor, deterministic=True, action_masks=MaskableAgent.mask_fn(self.env)
        )
        return action

    def optimize(self) -> None:
        """
        Optimize the agent with hyperparameters.
        """

        def objective(trial):
            learning_rate = trial.suggest_float("learning_rate", 1e-5, 1e-3, log=True)
            n_steps = trial.suggest_int("n_steps", 32, 1024, log=True)
            batch_size = trial.suggest_int("batch_size", 16, 256, log=True)
            gae_lambda = trial.suggest_float("gae_lambda", 0.8, 1.0)
            ent_coef = trial.suggest_float("ent_coef", 0.0, 0.02)
            vf_coef = trial.suggest_float("vf_coef", 0.1, 1.0)
            clip_range = trial.suggest_float("clip_range", 0.1, 0.3)
            max_grad_norm = trial.suggest_float("max_grad_norm", 0.1, 1.0)

            agent = MaskablePPOAgent(
                self.env_config,
            )

            model = MaskablePPO(
                MaskableActorCriticPolicy,
                agent.env,
                learning_rate=learning_rate,
                n_steps=n_steps,
                batch_size=batch_size,
                gae_lambda=gae_lambda,
                ent_coef=ent_coef,
                vf_coef=vf_coef,
                clip_range=clip_range,
                max_grad_norm=max_grad_norm,
                verbose=0,
                device=self.device,
            )

            agent.model = model
            agent.train()

            return np.mean(agent.evaluate().rewards)

        study = optuna.create_study(
            study_name=self.name,
            storage="sqlite:///optuna_study.db",
            load_if_exists=True,
            direction="maximize",
            pruner=optuna.pruners.HyperbandPruner(),
            sampler=optuna.samplers.TPESampler(),
        )

        study.optimize(
            objective,
            n_trials=N_TRAILS,
            n_jobs=-1,
            show_progress_bar=True,
            catch=(ValueError,),
        )

        self.optimized_hyperparameters = study.best_params

        self.model = MaskablePPO(
            MaskableActorCriticPolicy,
            self.env,
            **self.optimized_hyperparameters,
            verbose=0,
            tensorboard_log=f"./tensorboard/",
            device=self.device,
        )

    def export_hyperparameters(self, filename: str):
        """
        Export optimized learned_hyperparameters to a YAML file.
        """
        with open(filename, "w") as file:
            yaml.dump(self.optimized_hyperparameters, file)
```

</details>

### Evaluation MaskableRandom on ElectricityMarketEnv

### Evaluation A2C on ElectricityMarketEnv

### Evaluation MaskablePPO with default hyperparameters on ElectricityMarketEnv

### Evaluation MaskablePPO with optimized hyperparameters on ElectricityMarketEnv

### Adding expert knowledge to the masking function making learning more efficient

------------------------------------------------------------------------

<a
href="https://github.com/techofer/02360018_final_project/blob/main/electricity_market/player.py#L374"
target="_blank" style="float:right; font-size:smaller">source</a>

### expert_knowledge_action_masks

>  expert_knowledge_action_masks ()

<details open class="code-fold">
<summary>Exported source</summary>

``` python
def is_action_safe(self, action: int) -> bool:
    charge_amount = self._charge_amount(action)
    target_state_of_charge = self._current_state_of_charge + charge_amount
    low, high = self._battery_safe_range
    return high > target_state_of_charge > low


def expert_knowledge_action_masks(self) -> np.ndarray:
    mask = np.array(
        [
            self._is_action_valid(action) and self.is_action_safe(action)
            for action in range(self.action_space.n)
        ],
        dtype=bool,
    )
    if not np.any(mask):
        mask[len(mask) // 2] = True
    return mask
```

</details>

------------------------------------------------------------------------

<a
href="https://github.com/techofer/02360018_final_project/blob/main/electricity_market/player.py#L367"
target="_blank" style="float:right; font-size:smaller">source</a>

### is_action_safe

>  is_action_safe (action:int)

### Evaluation MaskableRandomAgent with Expert Knowledge on ElectricityMarketEnv

### Evaluation MaskablePPO with default hyperparameters and Expert Knowledge on ElectricityMarketEnv

### Evaluation MaskablePPO with optimized hyperparameters and Expert Knowledge on ElectricityMarketEnv
